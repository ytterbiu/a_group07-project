---
title: "The Statistical Properties of Nasdaq Composite Index Log-Returns"
subtitle: "SMM047 Group Coursework 2025-26 - Group 07"
author:
  - "Abdulrahman Alolyan"
  - "Benjamin Evans"
  - "Amogh Sharma"
  - "Nivetha Subbiah"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  bookdown::html_document2:
    self_contained: true
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
    df_print: paged
  # runtime: shiny
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    number_sections: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: true
# fontsize: 10pt
bibliography: references.bib
---

```{=latex}
\newpage
```

# Introduction

## Executive summary

Lorem ipsum dolor sit amet, consectetur adipiscing elit. In ullamcorper dapibus
ante, dignissim pharetra justo consequat nec. Curabitur tempus nulla in augue
auctor, imperdiet eleifend enim elementum. Nulla facilisi. Integer sed metus non
justo semper posuere eu sit amet odio. Nullam sodales auctor justo, convallis
tempor lectus feugiat ut. Quisque et ligula et tellus placerat consequat eu vel
magna. Vivamus et arcu nulla. Quisque ullamcorper eu odio nec facilisis. Proin
ac commodo nibh. In hac habitasse platea dictumst. Sed tincidunt porttitor
maximus. Nunc ullamcorper aliquet consequat. Vestibulum porta, justo auctor
imperdiet porta, nulla mi gravida tortor, et mattis tortor diam.

```{=html}
<!-- ## Key Information

### Background

This R Markdown document was created as part of a group assignment for SMM047 at Bayes Business School, City St George's, University of London in Term 1 2025-26.

-   *BE note: use `rmarkdown::render("cs1-group07.rmd", output_format = "all")` to render this document* -->
```

```{r asthetic-header, include=FALSE, echo=FALSE}
# ==============================================================================
# SMM047 Probability and Mathematical Statistics (Subject CS1)
# Group Coursework 2025-26
# Group:        Group 07
# Authors (in alphabetical order):
#   - Abdulrahman Alolyan
#   - Benjamin Evans
#   - Amogh Sharma
#   - Nivetha Subbiah
# Professor:    Dr Russell Gerrard
# Institution:  Bayes Business School - City St George's, University of London
# Date:         TBC
# Description:  Term 1 group project for SMM047 Probability and Mathematical
# Statistics (50% of coursework grade - 15% of module grade). The R code below
# has been exported directly from an R Markdown (.rmd) file.  Hence the knitr
# settings.
# Dependencies:
#   - quantmod
#   - xts
#   - zoo
#   - ggplot2
#   - dplyr
#   - tidyr
#   - patchwork
#   - kableExtra
#   - e1071
#   - summarytools
#   - nortest
#   - boot
#   - clipr
# ==============================================================================

```

```{r setup-knitr, include=FALSE, echo=FALSE}
#----------------------- Initial setup (knitr settings) -----------------------#
dir.create("fig", showWarnings = FALSE)

# Defaults common to all outputs
knitr::opts_chunk$set(
  echo     = TRUE,
  message  = FALSE,
  warning  = FALSE,
  fig.align = "center",
  out.width = "100%",
  fig.path  = "fig/",
  dpi       = 300
)

# Output-specific settings
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "pdf",
    fig.pos = "ht",
    out.extra = ""
  )
} else {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "svglite"  # or "png"
  )
}
```

```{r setup-qol, include=FALSE, echo=FALSE}
#----------------------------- Clean environment ------------------------------#
rm(list = ls()) # Remove all objects
graphics.off() # Close all graphical devices
cat("\014") # Clean console
```

```{r load-dependencies, include=FALSE, echo=FALSE}
#------------------- Load dependencies / external libraries -------------------#
library(quantmod)
library(xts) # for downloading
library(zoo) # for downloading

library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
library(kableExtra)

library(e1071) # statistical tests
library(summarytools) # statistical tests (but not used other than demo)
library(nortest) # normality test

library(boot) # bootstrapping

# for custom functions
library(clipr) # for banner_comment function qol to annotate code
```

```{r custom-functions, include=FALSE, echo=FALSE}
#---------------------------- Custom QOL functions ----------------------------#
#####################################
# function: banner comments (used to to section up code)
# Usage: banner_comment("Element 1: data cleaning") -> then ctrl + v (or cmd+v)
#####################################
banner_comment <- function(text, width = 80, border = "#", fill = "-") {
  txt <- paste0(" ", text, " ")
  inner_width <- width - 2 * nchar(border)
  banner_string <- ""
  
  if (inner_width <= nchar(txt)) {
    banner_string <- paste0(border, txt, border)
  } else {
    pad_total <- inner_width - nchar(txt)
    pad_left <- pad_total %/% 2
    pad_right <- pad_total - pad_left
    
    banner_string <- paste0(
      border,
      strrep(fill, pad_left),
      txt,
      strrep(fill, pad_right),
      border
    )
  }
  
  cat(banner_string, "\n")
  # copy banner to allow direct pasting (requires clipr)
  clipr::write_clip(banner_string)
  # avoid [1] when printing if want to manually copy
  invisible(banner_string)
}
#####################################
# function: format p-values for text 
# Usage (in-line): `r format_p_vals(ad_test_result$p.value)`
# Usage (console): format_p_vals(ad_test_result$p.value)
#####################################
format_p_vals <- function(p) {
  if (length(p) != 1L || is.na(p)) {
    stop("Error! p must be a single non-missing value")
  }
  if (p > 1) {
    stop("Error! Value greater than 1")
  }
  if (p < 0) {
    stop("Error! Value less than 0")
  }

  if (p >= 0.01) {
    paste0("= ", formatC(p, format = "f", digits = 2))
  } else if (p >= 0.001) {
    paste0("= ", formatC(p, format = "f", digits = 3))
  } else {
    "< 0.001"
  }
}
#####################################
# function: format confidence intervals for tables & text 
# Usage (in-line): `r format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])`
# Usage (console): format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])
#####################################
format_interval <- function(lower, upper, digits=3) {
  paste0("[", 
       formatC(lower, format = "f", digits = digits), ", ",
       formatC(upper, format = "f", digits = digits), 
       "]")
}
```

```{r Download-data, include=FALSE}
#---------------------------- Download / Load data ----------------------------#
# Daily adjusted closing values for ^IXIC over the period 1 January 2018 to 31
# December 2024 were extracted using `getSymbols` from the `quantmod` package.
# These data were then used as the basis for our statistical evaluation in this
# report.
# 
# Update 2025-11-21: included download for full IXIC data (used in appendix)
#
# Define inputs & date range
ticker <- "^IXIC"
study_from <- as.Date("2018-01-01")
study_to   <- as.Date("2024-12-31")

# All data (for appendix etc.)
full_from  <- as.Date("1971-01-01")
full_to    <- study_to

# Clean ticker for filename (remove ^ and any non-alphanumerics)
ticker_clean <- gsub("[^A-Za-z0-9]", "", ticker)

make_fname <- function(prefix, from, to) {
  sprintf(
    "%s_%s_%sto%s.csv",
    ticker_clean,
    prefix,
    format(from, "%Y%m%d"),
    format(to,   "%Y%m%d")
  )
}

load_from_csv <- function(path) {
  dat <- read.csv(path, stringsAsFactors = FALSE)
  if (!"Date" %in% names(dat)) {
    stop("csv file does not have expected 'Date' column.")
  }
  xts::xts(
    dat[, setdiff(names(dat), "Date"), drop = FALSE],
    order.by = as.Date(dat$Date)
  )
}

download_ixic <- function(ticker, from, to) {
  getSymbols(
    ticker,
    src         = "yahoo",
    from        = from,
    to          = to,
    auto.assign = FALSE
  )
}

# General "load or download + cache" helper
load_or_download_xts <- function(ticker, from, to, cache_file) {
  need_write <- FALSE

  if (file.exists(cache_file)) {
    dat <- tryCatch(
      load_from_csv(cache_file),
      error = function(e) {
        message("Cached file invalid, downloading fresh data: ", e$message)
        need_write <<- TRUE
        download_ixic(ticker, from, to)
      }
    )
  } else {
    need_write <- TRUE
    dat <- download_ixic(ticker, from, to)
  }

  if (need_write) {
    df <- data.frame(
      Date = index(dat),
      coredata(dat)
    )
    write.csv(df, cache_file, row.names = FALSE)
  }

  dat
}

# get full ixic data (1971-01-01 -> 2024-12-31)
fname_full <- make_fname("full", full_from, full_to)
IXIC_full  <- load_or_download_xts(ticker, full_from, full_to, fname_full)

# get subset for our date range (2018-01-01 -> 2024-12-31)
IXIC <- IXIC_full[paste0(study_from, "/", study_to)]
```

```{r Prepare data, echo=FALSE}
# First we prepare our data, getting the adjusted close price, applying a log
# transform, calculating the log-return increment, and removing NaN values.

# get adjusted close price
IXIC_ad <- Ad(IXIC)
# log transform
y_n <- log(IXIC_ad)
# log-return increment (z_n) - get difference in log transformed data
z_n <- diff(y_n)
# remove NA values
z_n <- na.omit(z_n)

# BE note:
# @hull2021options Hull builds their model (the one we are given in the
#   assessment specs) on the assumption that the stock price follows a process
#   driven by dz, where z is a Wiener Process (BE note: this is another name for
#   Brownian Motion). By definition, a Wiener process has independent
#   increments. This means the movement in the next millisecond is completely
#   mathematically independent of the movement in the previous millisecond. Eq1
#   is the discrete-time snapshot of the continuous Wiener process Hull
#   describes. Because the underlying dz has "hard-coded" independence, the 
#   daily returns inherit the assumption 
#   or tldr; -> whole model is built on Brownian motion.
#   
#   BE note to self 2: Geometric Brownian Motion assumes returns are independant
#   Sec 14.8 in Hull looks at Fractional Brownian motion which is different 
#   ('non markov') which I think means it is dependent (idea that the market 
#   has 'memory' whereas a markov frame of reference is that future price only
#   depends on current price, not path taken to get there*)
#   - Something to check during office hours (don't fully understand)

```

## Background

<!-- https://obl20.com/wp-content/uploads/2020/03/topic1a.pdf -->

The Nasdaq Composite Index tracks thousands of securities and serves as a
primary benchmark for the technology sector [@nasdaqcompfs]. This study analyses the
distributional properties of the index using daily log-returns derived from
adjusted closing prices. In statistical finance, log-returns are preferred over
simple arithmetic returns because they are time-additive and mathematically
symmetric.[^log-symmetry] These properties are essential for valid long-run
volatility modeling [@campbell1997econometrics].

In the standard asset-pricing model derived from Geometric Brownian Motion 
[@hull2021options, Sec. 14.7], the log-price of an asset evolves according to 
Equation \@ref(eq:zn),

\begin{equation}
  (\#eq:zn)
  \log X_{n} = \log X_{n-1} + Z_{n} 
\end{equation}

where $Z_n = \log(X_n / X_{n-1})$ denotes the daily log-return. Consistent with 
the properties of Wiener processes, the log-returns
$Z_n$ are assumed to be normally distributed with constant mean $\mu$, variance
$\sigma^{2}$, and are independent over time. This report uses the Nasdaq as a
case study to assess how well these four assumptions (normality, constant mean,
constant variance, and independence) hold in practice.

```{=latex}
\newpage
```

# Individual elements

## Data cleaning and standard test of normality {#elementOne}

<!-- ### Identifying and removing outliers -->

```{r initial-visualisation, fig.cap = "Nasdaq Composite price trend and daily volatility (2018–2024). The left figure shows the daily adjusted closing price. The right figure shows the daily log-returns, highlighting day-to-day percentage changes and volitility.", echo = FALSE, eval = FALSE, include = FALSE}
# BE note: set eval to false on 19/11/2025 following group meeting
# same length as IXIC_ad, first value NA

df_initial_ixic <- tibble(
  date = index(IXIC_ad),
  price = as.numeric(IXIC_ad),
  z_n = as.numeric(z_n)
) %>%
  drop_na(z_n)

date_range <- range(df_initial_ixic$date, na.rm = TRUE)

p1_initial_vis <- ggplot(df_initial_ixic, aes(date, price)) +
  theme_bw() +
  geom_line(linewidth = 0.5) +
  labs(
    title = "NASDAQ Composite\n (Adjusted Close)",
    x = "Date",
    y = "Adjusted Close ($)"
  )

p2_initial_vis <- ggplot(df_initial_ixic, aes(date, z_n)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  labs(
    title = expression(Log ~ Return ~ Increment ~ (z[n])),
    x = "Date",
    y = expression(z[n])
  )

# gridExtra::grid.arrange(p1, p2, ncol = 2)
(p1_initial_vis | p2_initial_vis) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )
```

```{r volitility-plot, fig.cap = "Nasdaq Composite price volitility.", echo = FALSE, eval = FALSE}
trading_period_days <- 60

# rolling mean return 
el1_rolling_return <- rollapply(
  z_n,
  width = trading_period_days,
  FUN   = mean,
  align = "right",
  fill  = NA
) #* 252

# rolling volatility 
el1_rolling_volatility <- rollapply(
  z_n,
  width = trading_period_days,
  FUN   = sd,
  align = "right",
  fill  = NA
) #* sqrt(252)

# combine into one xts and give sensible names
el1_xts <- merge(el1_rolling_return, el1_rolling_volatility)
colnames(el1_xts) <- c("rolling_return", "rolling_volatility")

# convert xts → data frame with explicit date column
el1_df <- fortify.zoo(el1_xts, name = "date")
# columns are now: date, rolling_return, rolling_volatility

# plot: 60-day annualised rolling mean log-return
p1_rr <- ggplot(el1_df, aes(x = date, y = rolling_return)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  labs(
    title = "60-day rolling mean log-return",
    x = "Date",
    y = expression(Annualised ~ mean ~ log ~ return)
  )

# plot: 60-day rolling volatility
p2_vol <- ggplot(el1_df, aes(x = date, y = rolling_volatility)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  labs(
    title = "60-day annualised rolling volatility",
    x = "Date",
    y = expression(Annualised ~ volatility)
  )

# combine with patchwork
(p1_rr | p2_vol) &
  theme(
    plot.title  = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )
```

```{r Element 1, echo=FALSE}
# ==============================================================================
#---------- Element 1: data cleaning and standard test of normality -----------#
# ==============================================================================
```

```{r Element 1 - cleaning, echo = FALSE}
# outlier_periods_to_remove <- c(
#   "2020-03-01/2020-04-17",
#   "2021-01-10/2021-03-20",
#   "2022-06-01/2022-07-05"
# )
outlier_periods_to_remove <- c(
  "2020-02-24/2020-03-23"
)
# set all values from outlier period to NA
y_n_clean <- y_n
for (p in outlier_periods_to_remove) {
  y_n_clean[p] <- NA
}
# calculate log-return increments
z_n_clean <- diff(y_n_clean)
z_n_clean_pre_na_drop <- z_n_clean
# remove NA increments
z_n_clean <- na.omit(z_n_clean)
```

```{r regions-to-be-removed, eval = FALSE, echo = FALSE, fig.width=6, fig.height=6, fig.cap = "Initial visualisation and regions selected for removal", }
outlier_df <- tibble(
  id     = seq_along(outlier_periods_to_remove),
  period = outlier_periods_to_remove
) %>%
  tidyr::separate(period, into = c("start", "end"), sep = "/", convert = TRUE) %>%
  mutate(
    start = as.Date(start),
    end   = as.Date(end),
    mid   = start + floor(as.numeric(end - start) / 2),
    label = as.character(id)
  )

df <- tibble(
  date  = index(IXIC_ad),
  price = as.numeric(IXIC_ad),
  z_n   = as.numeric(z_n)
) %>%
  drop_na(z_n)

range_p1 <- range(df$price, na.rm = TRUE)
range_p2 <- range(df$z_n,   na.rm = TRUE)

label_y_p1 <- range_p1[2] - 0.08 * diff(range_p1)
label_y_p2 <- range_p2[2] - 0.02 * diff(range_p2)
# label_y_p1 <- label_y_p1*1.02
# label_y_p2 <- label_y_p2*1.2

base_theme <- theme_bw() +
  theme(
    plot.title  = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

p1 <- ggplot(df, aes(date, price)) +
  geom_line() +
  labs(
    title = "NASDAQ Composite\n(Adjusted Close)",
    x = "Date", y = "Adjusted Close ($)"
  ) +
  base_theme +
  # vertical dotted lines
  geom_vline(
    data = outlier_df,
    aes(xintercept = start),
    linetype = "dotted"
  ) +
  geom_vline(
    data = outlier_df,
    aes(xintercept = end),
    linetype = "dotted"
  ) +
  # circles
  geom_point(
    data = outlier_df,
    aes(x = mid, y = label_y_p1),
    inherit.aes = FALSE,
    shape = 21,
    fill  = "white",
    color = "black",
    size  = 6,
    stroke = 0.6
  ) +
  # numbers
  geom_text(
    data = outlier_df,
    aes(x = mid, y = label_y_p1, label = label),
    inherit.aes = FALSE,
    vjust = 0.35,
    size = 3
  ) #+
  #scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)))

p2 <- ggplot(df, aes(date, z_n)) +
  geom_line() +
  labs(
    title = expression(Log~Return~Increment~(z[n])),
    x = "Date",
    y = expression(z[n])
  ) +
  base_theme +
  geom_vline(
    data = outlier_df,
    aes(xintercept = start),
    linetype = "dotted"
  ) +
  geom_vline(
    data = outlier_df,
    aes(xintercept = end),
    linetype = "dotted"
  ) #+
  # geom_point(
  #   data = outlier_df,
  #   aes(x = mid, y = label_y_p2),
  #   inherit.aes = FALSE,
  #   shape = 21,
  #   fill  = "white",
  #   color = "black",
  #   size  = 6,
  #   stroke = 0.6
  # ) +
  # geom_text(
  #   data = outlier_df,
  #   aes(x = mid, y = label_y_p2, label = label),
  #   inherit.aes = FALSE,
  #   vjust = 0.35,
  #   size = 3
  # ) #+
  #scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)))

(p1 / p2)
```

```{r raw-vs-clean, eval = FALSE, echo=FALSE, fig.cap = "Raw vs clean* TBC"}
# We can plot a quick visualisation to compare pre & post outlier removal:
df_raw <- tibble(
  date = index(z_n),
  z_n  = as.numeric(z_n)
)
df_clean <- tibble(
  date = index(z_n_clean_pre_na_drop),
  z_n  = as.numeric(z_n_clean_pre_na_drop)
)
ylim_all <- range(df_raw$z_n, df_clean$z_n, na.rm = TRUE)


p_raw <- ggplot(df_raw, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(pre exclusion)",
    x = "Date",
    y = expression(z[n])
  )

p_clean <- ggplot(df_clean, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(post exclusion)",
    x = "Date",
    y = expression(z[n])
  )

(p_raw | p_clean) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

```

```{r Element 1 pt2, echo = FALSE}
#----------------------------- Element 1 - Table ------------------------------#
# Skewness and excess kurtosis of the cleaned log-returns $z_n$ were computed
# using the `e1071` package in R with type = 1 [@R-e1071]. This corresponds to
# the classical moment-based definitions $m_3/m_2^{3/2}$ and $m_4/m_2{^2} - 3$
# described by @joanes1998, and used in SMM047 2025-26
# [@gerrard_seg2_notes_questions]. These measures are widely used in textbooks
# and provide a direct description of the shape of the empirical distribution.
# Alternative implementations, such as the package `summarytools`, were
# considered. This includes applying a apply small-sample bias corrections
# (“type 3” estimators), which lead to slightly different values but are
# asymptotically equivalent. Given our relatively large sample size and
# descriptive aim, the type 1 definitions are appropriate and fully adequate.

# e1071::skewness(z_n_clean, type = 1)
z_n_clean_numeric <- as.numeric(z_n_clean)

z_n_mean <- mean(z_n_clean_numeric)
z_n_vari <- var(z_n_clean_numeric)
z_n_skew <- e1071::skewness(z_n_clean_numeric, type = 1)
z_n_kurt <- e1071::kurtosis(z_n_clean_numeric, type = 1)

desc_z_n_clean <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness",
    "Excess Kurtosis"
  ),
  Value = c(
    z_n_mean,
    z_n_vari,
    z_n_skew,
    z_n_kurt
  )
) |>
  mutate(Value = sprintf("%.6f", Value))
```


```{r zn-desc-ad, echo=FALSE, message=FALSE, warning=FALSE, include = FALSE}
#- Element 1 - (bonus) Table with type 2 & 3 estimators (not used in report) --#
# BE note: changed to include = FALSE on 19/11/2025
z_n_clean_numeric <- as.numeric(z_n_clean)

desc_ad <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness (Type 1)",
    "Skewness (Type 2)",
    "Skewness (Type 3)",
    "Excess Kurtosis (Type 1)",
    "Excess Kurtosis (Type 2)",
    "Excess Kurtosis (Type 3)"
  ),
  Value = c(
    mean(z_n_clean_numeric),
    var(z_n_clean_numeric),
    e1071::skewness(z_n_clean_numeric, type = 1),
    e1071::skewness(z_n_clean_numeric, type = 2),
    e1071::skewness(z_n_clean_numeric, type = 3),
    # already excess kurtosis for type = 1
    e1071::kurtosis(z_n_clean_numeric, type = 1),
    e1071::kurtosis(z_n_clean_numeric, type = 2),
    e1071::kurtosis(z_n_clean_numeric, type = 3)
  )
) |>
  mutate(Value = sprintf("%.6f", Value))
kable(
  desc_ad,
  caption = "Additional descriptive statistics of cleaned log returns ($z_n$).",
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")
```


```{r Test of normality, echo=FALSE, include = FALSE}
ad_test_result <- ad.test(z_n_clean)
ad_test_result
```

Nasdaq Composite adjusted closing prices (2018–2024) were reviewed to identify
periods of volatility unrelated to 'legitimate' market activity (Figure
\@ref(fig:timeline-fig)).[^legit-market] Following numerical analysis and
qualitative review, only the onset of the COVID-19 pandemic
(24/02/2020-23/03/2020) was excluded as a non-representative outlier. Figure
\@ref(fig:initial-data-visualisation-volatility-and-regions-to-remove) shows the
Nasdaq adjusted close over this period and the period selected for exclusion.
Further details regarding the exclusion selection process are discussed in
Appendix \S\@ref(apd-outlier-selection).

```{r zn-desc, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  desc_z_n_clean,
  # caption = "Descriptive statistics of cleaned log returns ($z_n$).",
  caption = paste0(
    "Descriptive statistics of cleaned Nasdaq log-returns (n = ", 
    format(length(z_n_clean), big.mark = ","), 
    "). Skewness and kurtosis were calculated using Type 1 estimators. Given the large sample size, differences compared to bias-corrected (Type 2 or 3) estimators are negligible."
  ),
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(
      full_width = FALSE, 
      position = "center", 
      latex_options = "hold_position"
    )
```

As shown in Table \@ref(tab:zn-desc), the resulting daily log-returns ($Z_n$) 
exhibit pronounced excess kurtosis
($\gamma_2 =$ `r formatC(e1071::kurtosis(z_n_clean_numeric, type = 1), format = "fg", digits = 3)`). 
This leptokurtic behavior is consistent with the "stylized empirical facts" of 
financial time series described by @Cont01022001 and early findings by 
@Mandelbrot1963.[^cont2001quote]

Given the heavy-tailed nature of the data, the Anderson–Darling test was
selected to assess normality. The Anderson–Darling test is preferable to
alternatives, such as the Kolmogorov–Smirnov test, due to its increased
sensitivity to deviations in the tails of the distribution. The hypotheses are
defined as:

$$
\begin{cases}
H_0: Z_n \text{ is drawn from a normal distribution}\\
H_1: Z_n \text{ is not drawn from a normal distribution}
\end{cases}
$$

The test statistic was A = `r round(ad_test_result$statistic, 1)` 
(p `r format_p_vals(ad_test_result$p.value)`). We therefore reject the null
hypothesis ($H_0$). The Anderson-Darling test provides strong evidence that the 
Nasdaq log-returns are not normally distributed.


```{r timeline-fig, echo=FALSE, fig.cap="Notable Nasdaq market events from 2018-2024. Only the period associated with COVID-19 met the numerical criteria for a systemic outlier (see Appendix A for more information).", out.width="80%", fig.align='center', out.width="100%"}
knitr::include_graphics("fig/timeline2.pdf")
```


```{r initial-data-visualisation-volatility-and-regions-to-remove, eval = TRUE, echo = FALSE, fig.width=6, fig.height=7.0, fig.cap = "Nasdaq Composite price and volatility, 2018–2024. The high-volatility period associated with COVID-19 is indicated by the red shaded region (labeled 1). From top to bottom: (i) daily adjusted closing price; (ii) daily log-return increments; (iii) 20-day rolling volatility, measured as the standard deviation of daily log-returns over a 20-trading-day rolling window; and (iv) daily log-return increments after excluding the marked COVID-19 period.", fig.pos='!ht'}
#------------ Element 1 - Figure showing data pre & post-exclusion ------------#
trading_period_days <- 20

# trading period rolling log-return volatility (standard deviation)
el1_rolling_sd <- rollapply(
  z_n,
  width = trading_period_days,
  FUN   = sd,
  align = "right",
  fill  = NA
) # * sqrt(252) 
# BE note: have not converted to annualised here - saving for section 3/4
# It's challenging to convey this within the text limits for element 1.  Opted 
# to introduce later mathematically where we have a bit more space

# periods to remove
outlier_df <- tibble(
  id     = seq_along(outlier_periods_to_remove),
  period = outlier_periods_to_remove
) %>%
  tidyr::separate(period, into = c("start", "end"), sep = "/", convert = TRUE) %>%
  mutate(
    start = as.Date(start),
    end   = as.Date(end),
    mid   = start + floor(as.numeric(end - start) / 2),
    label = as.character(id)
  )
# outliner dotted lines for all plots
outlier_lines <- list(
  geom_vline(
    data = outlier_df, 
    aes(xintercept = start), 
    linetype = "dotted", 
    color = "red" # Added color here
  ),
  geom_vline(
    data = outlier_df, 
    aes(xintercept = end), 
    linetype = "dotted", 
    color = "red" # Added color here
  )
)
# testing using a shaded rectangle instead
outlier_shading <- geom_rect(
  data = outlier_df,
  aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),
  fill = "red", 
  alpha = 0.2,       # Makes it transparent
  inherit.aes = FALSE # Prevents conflicts with main plot aesthetics
)

# align price with z_n
IXIC_price_aligned <- IXIC_ad[index(z_n)]

# merge into one xts object (inner join on dates)
all_xts <- merge(
  IXIC_price_aligned,
  z_n,
  el1_rolling_sd
)
  # join = "inner"

colnames(all_xts) <- c("price", "z_n", "rolling_sd")

# convert to tibble
df <- tibble(
  date       = index(all_xts),
  price      = as.numeric(all_xts$price),
  z_n        = as.numeric(all_xts$z_n),
  rolling_sd = as.numeric(all_xts$rolling_sd)
)

range_p1 <- range(df$price, na.rm = TRUE)
range_p2 <- range(df$z_n,   na.rm = TRUE)

label_y_p1 <- range_p1[2] - 0.08 * diff(range_p1)
label_y_p2 <- range_p2[2] - 0.02 * diff(range_p2)

base_theme <- theme_bw() +
  theme(
    plot.title  = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

# initial visualisation p1: adjusted close with marked periods
initial_vis_p1 <- ggplot(df, aes(date, price)) +
  geom_line() +
  labs(
    title = "Nasdaq Composite (Adjusted Close)",
    x = NULL,
    y = "Adjusted close (USD)"
  ) +
  base_theme +
  outlier_shading +
  geom_point(
    data = outlier_df,
    aes(x = mid, y = label_y_p1),
    inherit.aes = FALSE,
    shape = 21,
    fill  = "white",
    color = "black",
    size  = 6,
    stroke = 0.6
  ) +
  geom_text(
    data = outlier_df,
    aes(x = mid, y = label_y_p1, label = label),
    inherit.aes = FALSE,
    vjust = 0.35,
    size = 3
  )

# initial visualisation p2: raw (pre-exclusion) daily log-return increments
initial_vis_p2 <- ggplot(df, aes(date, z_n)) +
  geom_line() +
  labs(
    title = "Daily Log-return Increments",
    x = NULL,
    y = expression(z[n])
  ) +
  base_theme +
  outlier_shading

# initial_vis p3: 20-day rolling log-return volatility (SD)
initial_vis_p3 <- ggplot(df, aes(date, rolling_sd)) +
  geom_line(na.rm = TRUE) +
  labs(
    title = paste0(
      "Nasdaq Composite Volatility (", 
      trading_period_days, 
      "-day rolling window)"
      ),
    x = NULL,
    y = bquote( sigma[n] ~ "(" * .(trading_period_days) * "-day rolling)")
  ) +
  base_theme +
  outlier_shading

# initial_vis_ p4: cleaned log-return series
df_clean_raw <- tibble(
  date = index(z_n_clean),
  z_n  = as.numeric(z_n_clean)
)

# Ensuring removed dates have z_n = NA, creating gap/gaps in the line
df_clean <- df %>%
  select(date) %>%
  left_join(df_clean_raw, by = "date")

# align y lim
ylim_all <- range(df$z_n, df_clean$z_n, na.rm = TRUE)

initial_vis_p4 <- ggplot(df_clean, aes(date, z_n)) +
  geom_line() +                 
  base_theme +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Daily Log-return Increments (Post Exclusion)",
    x = "Date",
    y = expression(z[n])
  )

# 4-panel figure
(initial_vis_p1 / initial_vis_p3 / initial_vis_p2 / initial_vis_p4)
```

```{=latex}
\clearpage
\newpage
```

## Investigation of normality by resampling {#elementTwo}

<!-- ### Instructions (to be removed later) -->

<!-- _You are going to investigate whether the difference between your sample and a -->
<!-- sample from a normal distribution becomes apparent when constructing confidence -->
<!-- intervals for sample skewness._ -->

<!-- _(a) Simulate a sample of size 13 from a Normal distribution with mean and -->
<!-- variance equal to the sample mean and variance you have just calculated. -->
<!-- Calculate the sample skewness from this simulated sample. Repeat this until you -->
<!-- have generated 50,000 values of_ $\hat{\gamma}_1$. -->

<!-- _(b) Use resampling to obtain a sample of size 13 from your z-data. Calculate -->
<!-- the sample skewness from this bootstrap sample. Repeat this until you have -->
<!-- generated 50,000 values of_ $\hat{\gamma}_1$. -->

<!-- _(c) Use suitable illustrations to investigate the differences between the two -->
<!-- sets of_ $\hat{\gamma}$ values. Comment on what you observe and whether the -->
<!-- outcome of your tests confirms the result of the test of normality. -->

<!-- ### Simulating skewness from resampling a normal distribution & from $z$-data using bootstrapping -->

```{r Element 2, echo=FALSE}
# ==============================================================================
#------------ Element 2: investigation of normality by resampling -------------#
# ==============================================================================
```
```{r Element 2 pt1 - Simulating skewness from a normal distribution, cache = FALSE, echo = FALSE}
# set random seed
set.seed(1234)

# set parameters from z_data (mean & st-dev)
z_mean <- mean(z_n_clean)
z_sd <- sd(z_n_clean)
N_reps <- 50000
n_sample_size <- 13

# Simulate 50,000 skewness values using sample size of 13
#   Generate 13 values normally distributed with (z_mean, z_sd) 
#   Calculate skewness of these 13 values
#   Repeat process 50,000 times
#   Tells one how much noise one would expect given this small sample size (13)
skew_normal_sim <- replicate(
  N_reps, e1071::skewness(rnorm(n_sample_size, z_mean, z_sd), type = 1)
  )

# bootstrapping skewness simulation
#   Pick 13 values from z_n_clean (with replacement)
#   Calculate skewness of these 13 values
#   Repeat process 50,000 times
#   Looking at skewness if we just take 13 random days of data
skew_bootstrap <- replicate(
  N_reps, e1071::skewness(sample(z_n_clean, n_sample_size, replace = TRUE), type = 1)
  )

#------------ basic
skew_normal_sim_median <- median(skew_normal_sim)
skew_normal_bootstrap  <- median(skew_bootstrap)
sd_normal_sim <- sd(skew_normal_sim)
sd_normal_boo <- sd(skew_bootstrap)
el2_original_skew <- e1071::skewness(z_n_clean, type = 1)
el2_bias <- mean(skew_bootstrap) - el2_original_skew

#------------ statistics/metrics
# calculating quantile based confidence interval
ci_95_skew_zn_boot_quantile <- quantile(skew_bootstrap, probs = c(0.025, 0.975))
ci_95_skew_normals_quantile <- quantile(skew_normal_sim, probs = c(0.025, 0.975))
# 
# print(ci_95_skew_zn_boot_quantile)
# print(ci_95_skew_normals_quantile)
```

```{r el2-full-bootstrap, echo=FALSE, cache = FALSE}

# Using open source: https://gitlab.com/scottkosty/bootstrap/-/blob/master/R/bcanon.R
#
# Takes a vector of observations x, the number of bootstrap samples to take,
# an estimator plus additional parameters for it, and confidence levels for
# the output intervals
"bcanon" <- function(x,nboot,theta,...,alpha =
                     c(.025,.05,.1,.16,.84,.9,.95,.975)) {
    if (!all(alpha < 1) || !all(alpha > 0))
      stop("All elements of alpha must be in (0,1)")

    # NB. these lines check that nboot > (1 / alpha) and because otherwise
    # you need more samples to get a somewhat useful confidence interval.
    alpha_sorted <- sort(alpha)
    if (nboot <= 1/min(alpha_sorted[1],1-alpha_sorted[length(alpha_sorted)]))
      warning("nboot is not large enough to estimate your chosen alpha.")

    # unrelated to the actual bootstrapping
    call <- match.call()

    # compute theta(x) of the samples and resample the data nboot times
    n <- length(x)
    thetahat <- theta(x,...)
    bootsam<- matrix(sample(x,size=n*nboot,replace=TRUE),nrow=nboot)

    # compute theta for each sample and compute the quartile of the fraction
    # below our original estimate under a normal distribution
    thetastar <- apply(bootsam,1,theta,...)
    z0 <- qnorm(sum(thetastar<thetahat)/nboot)

    # get a jackknife estimate for theta to compute the acceleration factor
    u <- rep(0,n)
    for(i in 1:n){
        u[i] <- theta(x[-i],...)
    }
    uu <- mean(u)-u
    acc <- sum(uu*uu*uu)/(6*(sum(uu*uu))^1.5)

    # compute the actual distribution that we are taking the quantiles of to
    # create the confidence interval
    zalpha <- qnorm(alpha)

    tt <- pnorm(z0+ (z0+zalpha)/(1-acc*(z0+zalpha)))

    confpoints <- quantile(x=thetastar,probs=tt,type=1)

    # and now just some logic for outputting it
    names(confpoints) <- NULL
    confpoints <- cbind(alpha,confpoints)
    dimnames(confpoints)[[2]] <- c("alpha","bca point")
    return(list(confpoints=confpoints,
                z0=z0,
                acc=acc,
                u=u,
                call=call))
}
```

```{r bca-interval-bcanon, echo=FALSE, cache = FALSE}

#------------ BCa interval for skewness using bcanon
# bca_skew <- bcanon(
#   x     = z_n_clean,
#   nboot = N_reps,
#   alpha = c(.025,.975),
#   theta = e1071::skewness,
#   type  = 1
# )
# 
# # calculating quantile based confidence interval
# print(ci_95_skew_zn_boot_quantile)
# print(ci_95_skew_normals_quantile)
# 
# alpha and corresponding BCa points
# print(bca_skew$confpoints)

# quantile(skew_boot_full, c(0.025, 0.975))

#------------ 95% CIs for comparison ----------

# Normal-based parametric CI via simulation, n = 13
el2_ci_normal_95 <- quantile(skew_normal_sim, probs = c(0.025, 0.975))

# Bootstrap percentile CI from empirical data, n = 13
el2_ci_boot_95 <- quantile(skew_bootstrap, probs = c(0.025, 0.975))

# (bonus) bootstrap 9% CI for skewness using full data, n = ~1737
# using standard percentile
skew_boot_full <- replicate(
  N_reps,
  e1071::skewness(sample(z_n_clean, length(z_n_clean), replace = TRUE), type = 1)
)
el2_ci_boot_full_perc <- quantile(skew_boot_full, probs = c(0.025, 0.975))

# using bcanon BCa
bca_skew <- bcanon(
  x     = z_n_clean,
  nboot = N_reps,
  theta = e1071::skewness,
  type  = 1,
  alpha = c(0.025, 0.975)
)
el2_ci_boot_full_bca <- bca_skew$confpoints[, "bca point"]

ci_table <- data.frame(
  Dataset = c(
    "Small Sample (n=13)",
    "Small Sample (n=13)",
    "Full Data (n=1737)",
    "Full Data (n=1737)"
  ),
  Method = c(
    "Simulated Normal (Control)",
    "Bootstrap Percentile",
    "Bootstrap Percentile",
    "BCa Bootstrap (Robust)"
  ),
  Lower = c(
    el2_ci_normal_95[1],
    el2_ci_boot_95[1],        
    el2_ci_boot_full_perc[1],
    el2_ci_boot_full_bca[1]
  ),
  Upper = c(
    el2_ci_normal_95[2],
    el2_ci_boot_95[2],        
    el2_ci_boot_full_perc[2],
    el2_ci_boot_full_bca[2]
  )
)

ci_table_2_digits <- 2
ci_table_2 <- data.frame(
  Dataset = c(
    "Small Sample",
    "Small Sample",
    "Full Data",
    "Full Data"
  ),
  Method = c(
    "Simulated Normal",
    "Bootstrap Percentile",
    "Bootstrap Percentile",
    "Bootstrap BCa"
  ),
  N = c(
    n_sample_size,        
    n_sample_size,        
    length(z_n_clean),    
    length(z_n_clean)     
  ),
  `95 percent CI` = c(
    format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2], ci_table_2_digits),
    format_interval(el2_ci_boot_95[1], el2_ci_boot_95[2], ci_table_2_digits),
    format_interval(el2_ci_boot_full_perc[1], el2_ci_boot_full_perc[2], ci_table_2_digits),
    format_interval(el2_ci_boot_full_bca[1], el2_ci_boot_full_bca[2], ci_table_2_digits)
  ),
  check.names = FALSE
)
```

To assess the normality assumption under small-sample
constraints (n=13), the sampling distribution of the coefficient of
skewness ($\hat{\gamma}_1$) was simulated using 50,000 replications. This
process involved generating a baseline control from a theoretical normal
distribution and comparing it against an empirical set bootstrapped directly
from the Nasdaq log-returns.

Figure \@ref(fig:skewness-densities) compares a theoretical normal baseline
against an empirical bootstrap from the observed data. The theoretical
distribution is centred at zero with a 95% Confidence Interval (CI) of 
`r format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2], 2)`. In contrast, 
the empirical distribution exhibits a negative shift and notably heavier tails,
producing a wider 95% CI of 
`r format_interval(el2_ci_boot_95[1], el2_ci_boot_95[2], 2)`. 
While this interval includes zero, this likely reflects
the low statistical power of the sample size rather than underlying normality
[@bickel1997resampling].

To address this limitation, the full dataset (n=`r length(z_n_clean)`) was
analysed using the Bias-Corrected and accelerated (BCa) bootstrap. Unlike
standard percentile methods which assume symmetry, the BCa algorithm
[@efron1993introduction] (implemented in R by @kosty2019bcanon) adjusts for the
estimator’s inherent bias and skewness. This robust analysis returned a 95% CI
of
`r format_interval(el2_ci_boot_full_bca[1], el2_ci_boot_full_bca[2], 3)`. 
The exclusion of zero confirms significant negative skewness, corresponding to 
a risk of extreme negative events. In a random 13-day observation window, this 
finding was masked by the high noise inherent in small samples.

```{r skewness-densities, echo=FALSE, fig.cap = paste0("Sampling distributions of the skewness estimates for ",N_reps, " samples of size n=", n_sample_size, " taken from a Normal model (blue) and from the empirical log-returns $z_n$ via bootstrap (red)."), fig.pos='!ht'}

# bootstrapping skewness simulation
skew_df <- data.frame(
  skewness = c(skew_normal_sim, skew_bootstrap),
  Source   = factor(
    rep(c("Normal model", "Bootstrap from data"),
        each = N_reps)
  )
)

ggplot(skew_df, aes(x = skewness, fill = Source)) +
  geom_density(alpha = 0.4) +
  theme_bw() +
  labs(
    title = expression("Sampling distributions of " * hat(gamma)[1] *
                       " for n = 13"),
    x     = expression(hat(gamma)[1]),
    y     = "Density"
  )
```


<!-- [@efronbootstrap1979] -->
<!-- [@efron1987better] -->
<!-- [@bickel1997resampling] -->
<!-- [@kosty2019bcanon] -->
<!-- [@efron1993introduction] -->

```{r skewness-95CI, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
kable(
  ci_table,
  caption = "Comparison of 95% Confidence Intervals for skewness across sample sizes and methods. The ",
  booktabs = TRUE,
  align = c("l", rep("r", 3)),
  digits = 3,
  row.names = FALSE,
  escape = FALSE
) |>
  add_header_above(
    c(" " = 2, "95 percent CI" = 2),
    escape = FALSE
  ) |>
  kable_styling(full_width = FALSE, position = "center")
```

```{r skewness-95CI-2, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  ci_table_2,
  caption = "Comparison of 95 percent confidence intervals for skewness across sample sizes and methods. The simulated normal sample is a baseline control derived from a theoretical normal distribution with a small sampling size, the bootstrap percentile intervals are calculated directly from the empirical 2.5th and 97.5th quantiles, while the Bootstrap BCa interval employs bias correction and acceleration to adjust for the inherent asymmetry of the estimator, offering the most robust metric for the full dataset.",
  booktabs = TRUE,
  align = c("l", "l", "r", "l"),
  digits = 3,
  row.names = FALSE,
) %>%
  kable_styling(
    full_width = FALSE, 
    position = "center",
    latex_options = "hold_position"
    )
```


<!-- ```{=latex} -->
<!-- \clearpage -->
<!-- \newpage -->
<!-- ``` -->

## Investigation of constant mean {#elementThr}

<!-- ### Instructions (to be removed later) -->

<!-- _Divide your_ $z$ data into 14 subsamples, each representing six months. (If -->
<!-- your data cleansing has removed all or most of one six-month period, just omit -->
<!-- it from this analysis.) Compare the means of your subsamples using an -->
<!-- appropriate illustration, supported by at least one statistical test. [Note that -->
<!-- Analysis of Variance (ANOVA) is based on an assumption of Normality: if you -->
<!-- decide to use ANOVA for your test you will need to comment on the Normality, or -->
<!-- otherwise, of your 7 subsamples.] -->

<!-- ### Visual comparison of means -->

```{r Element 3, echo=FALSE}
# ==============================================================================
#----------------- Element 3: investigation of constant mean ------------------#
# ==============================================================================
```

```{r Element 3 pt1 - setup, echo = FALSE}
# # ---------------- attempt 01: using flexible 6 month windows
# # split z data into 14 subsamples
# z_list <- split(z_n_clean, f = "months", k = 6)
#
# # drop values if all or most of 1 6M period have been removed
# # BE note: have just removed fully blank values, but need to tweak this
# z_list <- z_list[lengths(z_list) > 0]

# ---------------- attempt 02: new (using hard 6 month cut-offs)

# ensure we have dates
dates <- index(z_n_clean)

# start - first 6-month block that contains the first date
first_date <- min(dates)
fy <- as.integer(format(first_date, "%Y"))
fm <- as.integer(format(first_date, "%m"))
start0 <- if (fm <= 6) {
  as.Date(sprintf("%d-01-01", fy))
} else {
  as.Date(sprintf("%d-07-01", fy))
}

# end - first boundary after the last date
last_date <- max(dates)
ly <- as.integer(format(last_date, "%Y"))
lm <- as.integer(format(last_date, "%m"))
end_next <- if (lm <= 6) {
  as.Date(sprintf("%d-07-01", ly))
} else {
  as.Date(sprintf("%d-01-01", ly + 1))
}

# fixed 6-month breakpoints
breaks <- seq(start0, end_next, by = "6 months")

# nice labels "01 Jan 2018 - 30 Jun 2018" etc.
start_labels <- breaks[-length(breaks)]
end_labels   <- breaks[-1] - 1

period_labels <- paste(
  format(start_labels, "%b %Y"),
  "-",
  format(end_labels,   "%b %Y")
)

# assign each observation to a 6-month bin (right-open [start, end])
Period <- cut(
  dates,
  breaks = breaks,
  right  = FALSE,
  labels = period_labels,
  include.lowest = TRUE
)

# create list of subsamples
z_list <- split(z_n_clean, Period)

# drop empty periods (where cleaning removed all data)
z_list <- z_list[lengths(z_list) > 0]

combined_df_el3 <- bind_rows(lapply(seq_along(z_list), function(i) {
  data.frame(
    Period  = factor(names(z_list)[i], levels = names(z_list)),
    z_value = as.numeric(z_list[[i]])
  )
}))
```

```{r element3-days-per-six-month-period, echo=FALSE, message=FALSE, warning=FALSE}
#---------- Element 4: check & count observations in 6-month periods ----------#
# Count observations in each 6-month period
days_per_period <- sapply(z_list, NROW)

period_labels <- names(days_per_period)

# split into start/end strings
start_dates <- sub(" -.*", "", period_labels)
end_dates   <- sub(".*- ", "", period_labels)

days_table <- data.frame(
  `Start date`             = start_dates,
  `End date`               = end_dates,
  `Number of data entries` = as.integer(days_per_period),
  check.names = FALSE
)
```

<!-- ### Statistical test -->

```{r kruskal-test, echo = FALSE}
kruskal_res <- kruskal.test(z_value ~ Period, data = combined_df_el3)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kruskal_df <- data.frame(
  Test = "Kruskal–Wallis rank sum test",
  Df = kruskal_res$parameter,
  Statistic = unname(kruskal_res$statistic),
  p.value = kruskal_res$p.value
)

kruskal_df$Statistic <- sprintf("%.3f", kruskal_df$Statistic)
kruskal_df$p.value <- formatC(kruskal_df$p.value, format = "fg", digits = 4)
# The interpretation of the Kruskal-Wallis test depends on the homogeneity of
# variance across groups. We test assumption of constant variance in
# \S\@ref(elementFou) so not safe (I think) to assume it here.
# We instead rely on the strict assumption that the distributions share 
# identical shapes. The Kruskal-Wallis test should therefore be interpreted in 
# its most general form 
# -> assessing differences in mean ranks (stochastic dominance) rather than a 
# whether the medians are equal
# 
# Stochastic dominance:
# If Group A stochastically dominates Group B, it implies that if you picked 
# one random number from A and one random number from B, the number from A 
# is likely to be larger than B more than 50% of the time.
# tldr; because some boxplots are fat (high variance) and some are thin, we 
# can't just talk about the median - instead want to say something like: 
# "This group generally tends to produce higher numbers than that group"
```

```{r kruskal-res, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
kable(
  kruskal_df,
  caption = "Kruskal–Wallis test outcome for differences in log-return distributions across six-month periods.",
  booktabs = TRUE,
  align = c("l", "r", "r", "r"),
  row.names = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")
```

To evaluate the assumption that the log-returns $z_n$ possess a constant mean,
$\mu$, the log-returns data were partitioned into fourteen 6-month periods. As
shown in Table \@ref(tab:daysPerSixMonthPeriod), the Jan 2020 - Jul 2020 window
contains fewer observations (n=`r min(days_per_period)`) following data
cleaning. However, all subsamples remain sufficiently large
(n>`r floor(min(days_per_period) / 10) * 10`) 
to yield reliable statistical estimates.

Given the significant departures from normality established in
\S\@ref(elementOne) & \@ref(elementTwo), parametric methods, such as Analysis of
Variance are inappropriate. The Kruskal-Wallis rank sum test was 
implemented to determine whether the log-returns across the 6-month periods 
originate from the same distribution [@hollander2013nonparametric]. 

The interpretation of the Kruskal-Wallis test depends on the homogeneity of
variance across groups. The assumption of constant variance is tested in
\S\@ref(elementFou) (and may not hold given the volatility clustering observed
in Figure \@ref(fig:element3-boxplot)), so one cannot rely on the strict
assumption that the distributions share identical shapes. The Kruskal-Wallis 
test is therefore interpreted in its most general form: assessing
differences in mean ranks (stochastic dominance) rather than a strict equality
of medians. The hypotheses are defined as:

$$
\begin{cases}
H_0: \text{The distributions of the 14 samples are identical} \\
H_1: \text{At least one subsample stochastically dominates another} \\ 
\qquad\, \text{(i.e., tends to produce systematically larger or smaller values)}
\end{cases}
$$

The Kruskal–Wallis test statistic was 
$\chi^{2}_{\text{`r signif(kruskal_res$parameter[[1]], 3)`}}$ =
`r signif(kruskal_res$statistic[[1]], 4)`
(p `r format_p_vals(kruskal_res$p.value)`). There is insufficient evidence to reject the null hypothesis at the 95%
confidence level.

There are no statistically significant differences in the mean ranks of the
log-returns across the fourteen 6-month periods. This suggests that while the 
volatility (variance) may fluctuate over time, the location of the distribution 
remains stable. The Nasdaq does not exhibit 6-month periods where log-returns are 
systematically higher or lower on average, supporting the assumption of a 
constant mean $\mu$. Crucially, the visual evidence of volatility clustering 
indicates that this stability does not extend to the variance.

```{r element3-boxplot, echo=FALSE, fig.height=3.4, fig.pos='!ht', fig.cap = "Boxplots showing the distribution of Nasdaq log-returns partitioned into fourteen six-month intervals (Jan 2018 – Dec 2024)."}
ggplot(combined_df_el3, aes(x = Period, y = z_value)) +
  geom_boxplot() +
  theme_bw() +
  labs(
    title = "Log-returns over six-month periods",
    x = "Six-month period",
    y = expression(z[n])
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r daysPerSixMonthPeriod, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  days_table,
  col.names = c(
    "Start date",
    "End date",
    "Number of data points post cleaning"
  ),
  caption = "Number of non-missing log-return observations in each six-month period.",
  booktabs = TRUE,
  longtable = TRUE,
  align = c("r","r", "c")
) |>
  kable_styling(
    full_width = FALSE,
    position = "center"
  )
```

```{=latex}
\clearpage
\newpage
```

## Investigation of constant variance {#elementFou}

<!-- ### Identifying subsamples with max/min variance (chapter 4 in CS1) -->
```{r Element 4, echo=FALSE}
# ==============================================================================
#--------------- Element 4: investigation of constant variance ----------------#
# ==============================================================================
```

```{r Element 4 pt1 - processing, echo=FALSE}
variances <- sapply(z_list, var, na.rm = TRUE)
min_var_loc <- which.min(variances)
max_var_loc <- which.max(variances)

# set up variance dataframe
var_df <- data.frame(
  Period   = factor(names(z_list), levels = names(z_list)),
  Variance = as.numeric(variances)
)

# !changed to *252: extract the subsamples as numeric vectors & drop NA
min_var_sample <- na.omit(as.numeric(z_list[[min_var_loc]]))*252
max_var_sample <- na.omit(as.numeric(z_list[[max_var_loc]]))*252
min_var_sample <- na.omit(as.numeric(z_list[[min_var_loc]]))
max_var_sample <- na.omit(as.numeric(z_list[[max_var_loc]]))

# create bootstrap statistic function to get sample variance of resampled data
boot_var <- function(data, indices) {
  var(data[indices])
}

# run separate bootstrapping for both subsamples
boot_var_min <- boot(
  data      = min_var_sample,
  statistic = boot_var,
  R         = 5000
)

boot_var_max <- boot(
  data      = max_var_sample,
  statistic = boot_var,
  R         = 5000
)

# get CI using Bias-Correct and Accelerated (bca) interval
# (this adjusts for both bias and skewness in bootstrap distribution)
var_ci_min_95 <- boot.ci(boot_var_min, type = "bca", conf = 0.95)
var_ci_max_95 <- boot.ci(boot_var_max, type = "bca", conf = 0.95)
# 99% CI for interest
var_ci_min_99 <- boot.ci(boot_var_min, type = "bca", conf = 0.99)
var_ci_max_99 <- boot.ci(boot_var_max, type = "bca", conf = 0.99)
```

```{r Element 4 pt2 - table, echo=FALSE}
# Period labels for min/max variance subsamples
min_period <- names(z_list)[min_var_loc]
max_period <- names(z_list)[max_var_loc]

# Extract BCa 95% CI limits from boot.ci output
# (bca columns: conf, lower, upper, etc. We want the low & high limits.)
var_ci_min_bca <- var_ci_min_95$bca[4:5]
var_ci_max_bca <- var_ci_max_95$bca[4:5]

# getting end date
# start date of each block
start_dates <- as.Date(sapply(z_list, function(x) index(x)[1]))

# last observed date in each block
last_dates  <- as.Date(sapply(z_list, function(x) index(x)[NROW(x)]))

# end date of each block:
# - for all but last: day before next block's start
# - for last: its own last date
end_dates <- c(start_dates[-1] - 1, last_dates[length(last_dates)])
period_labels <- paste(
  base::format(start_dates, "%b %Y"),
  "-",
  base::format(end_dates, "%b %Y")
)

var_df <- data.frame(
  Period   = factor(period_labels, levels = period_labels),
  Variance = as.numeric(variances)
)

min_period_label <- period_labels[min_var_loc]
max_period_label <- period_labels[max_var_loc]

```

```{r element4-box-bar, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions and variability of cleaned log-returns across six-month periods. Top: violin and boxplot summaries of $z_n$ for each period. Bottom: corresponding sample variances $s^2$, highlighting relative differences in spread."}
# replotting box plot from investigation of constant mean above new variance bar
# chart

# boxplot
p_box_el4 <- ggplot(combined_df_el3, aes(x = Period, y = z_value)) +
  geom_boxplot(width = 0.75, outlier.size = 0.7, linewidth   = 0.3) +
  theme_bw() +
  labs(
    title = "Log-returns over six-month periods",
    x = "Six-month period",
    y = expression(z[n])
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_blank(),
    axis.ticks.x    = element_blank(),
  )

p_var_el4 <- ggplot(var_df, aes(x = Period, y = Variance, fill = Period)) +
  # scale_fill_viridis_d() +
  geom_col(fill = "white", colour = "black", width = 0.75, linewidth   = 0.3) +
  theme_bw() +
  labs(
    title = "Sample variance of log-returns by six-month period",
    x = "Six-month period",
    y = expression(s^2)
  ) +
  # scale_y_continuous(limits = c(0, 5e-04), expand = c(0,0)) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_text(angle = 45, hjust = 1),
    legend.position = "none",
  )

# Stack vertically with aligned x-axis
(p_box_el4 / p_var_el4) +
  plot_layout(heights = c(1, 1))
```

```{r element4-vio-bar, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions and variability of cleaned log-returns across six-month periods. Top: boxplot summaries of $z_n$ for each period. Bottom: corresponding sample variances $s^2$, highlighting relative differences in spread."}

p_violin <- ggplot(combined_df_el3, aes(x = Period, y = z_value, fill = Period)) +
  geom_violin(trim = TRUE, colour = "grey20", alpha = 0.4) +
  geom_boxplot(
    width = 0.2,
    outlier.size = 0.5,
    fill = "white",
    colour = "black"
  ) +
  scale_fill_viridis_d() +
  theme_bw() +
  labs(
    title = "Distribution of log-returns across six-month periods",
    x = NULL,
    y = expression(z[n])
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_blank(),
    axis.ticks.x    = element_blank(),
    legend.position = "none"
  )

p_var <- ggplot(var_df, aes(x = Period, y = Variance, fill = Period)) +
  geom_col(alpha = 0.95) +
  scale_fill_viridis_d() +
  theme_bw() +
  labs(
    title = "Sample variance of log-returns by six-month period",
    x = "Six-month period",
    y = expression(s^2)
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

# Stack vertically with aligned x-axis
(p_violin / p_var) +
  plot_layout(heights = c(1, 1))
```

<!-- ### Bootstrapping confidence intervals for $\sigma^2$ -->

```{r var-ci, echo=FALSE, message=FALSE, warning=FALSE}
# Create summary table
var_ci_table <- data.frame(
  Subsample = c("Min variance", "Max variance"),
  Period = c(min_period_label, max_period_label),
  s2        = c(var(min_var_sample),
                var(max_var_sample)),
  CI_lower  = c(var_ci_min_bca[1],
                var_ci_max_bca[1]),
  CI_upper  = c(var_ci_min_bca[2],
                var_ci_max_bca[2])
)

fmt <- function(x) formatC(x, format = "f", digits = 6)

var_ci_table_disp <- var_ci_table %>%
  mutate(
    s2       = paste0("$", fmt(s2), "$"),
    CI_lower = paste0("$", fmt(CI_lower), "$"),
    CI_upper = paste0("$", fmt(CI_upper), "$")
  )

kable(
  var_ci_table,
  col.names = c(
    "Subsample",
    "Six-month period",
    "Sample variance",
    "95\\% CI lower",
    "95\\% CI upper"
  ),
  booktabs = TRUE,
  align = c("l", "l", "r", "r", "r"),
  escape = FALSE,
  digits = 6,
  caption = "Bootstrap bias-corrected and accelerated 95\\% confidence intervals for the sample variance $s^2$ in the six-month periods with minimum and maximum sample variance."
) %>%
  kable_styling(full_width = FALSE, position = "center")

# kable(
#   var_ci_table,
#   caption = "Bootstrap BCa 95\\% confidence intervals for the variance $s^2$ in the six-month periods with the smallest and largest sample variance of log-returns.",
#   booktabs = TRUE,
#   align = c("l", "r", "r", "r"),
#   escape = FALSE
# ) |>
#   kable_styling(full_width = FALSE, position = "center")
```

```{r testing-anualised-data, echo=FALSE}
# Daily variance per period, then annualise
variances_daily <- sapply(z_list, var, na.rm = TRUE)
variances_ann   <- variances_daily * 252

min_var_loc <- which.min(variances_ann)
max_var_loc <- which.max(variances_ann)

# Period labels
start_dates <- as.Date(sapply(z_list, function(x) index(x)[1]))
last_dates  <- as.Date(sapply(z_list, function(x) index(x)[NROW(x)]))
end_dates   <- c(start_dates[-1] - 1, last_dates[length(last_dates)])

period_labels <- paste(
  base::format(start_dates, "%b %Y"), "-",
  base::format(end_dates,  "%b %Y")
)

# min/max periods (still daily reforms)
min_var_sample <- na.omit(as.numeric(z_list[[min_var_loc]]))
max_var_sample <- na.omit(as.numeric(z_list[[max_var_loc]]))

# multiply bu 252
var_ann_stat <- function(x, idx) var(x[idx]) * 252

# iid bootstrap
boot_min <- boot(data = min_var_sample, statistic = var_ann_stat, R = 5000)
boot_max <- boot(data = max_var_sample, statistic = var_ann_stat, R = 5000)

# BCa 95% CIs 
ci_min_95 <- boot.ci(boot_min, type = "bca", conf = 0.95)$bca[4:5]
ci_max_95 <- boot.ci(boot_max, type = "bca", conf = 0.95)$bca[4:5]

# annualised point estimates
s2_min_ann <- var(min_var_sample) * 252
s2_max_ann <- var(max_var_sample) * 252

min_period_label <- period_labels[min_var_loc]
max_period_label <- period_labels[max_var_loc]
```

```{r testing-table-rename, echo=FALSE}
var_ci_table <- data.frame(
  Subsample = c("Min variance", "Max variance"),
  Period    = c(min_period_label, max_period_label),
  s2        = c(s2_min_ann, s2_max_ann),
  CI_lower  = c(ci_min_95[1], ci_max_95[1]),
  CI_upper  = c(ci_min_95[2], ci_max_95[2])
)

fmt <- function(x) formatC(x, format = "f", digits = 6)

var_ci_table_disp <- var_ci_table |>
  dplyr::mutate(
    s2       = paste0("$", fmt(s2), "$"),
    CI_lower = paste0("$", fmt(CI_lower), "$"),
    CI_upper = paste0("$", fmt(CI_upper), "$")
  )

kable(
  var_ci_table,
  col.names = c("Subsample", "Six-month period",
                "Annualised variance $s^2$ (×252)",
                "95\\% CI lower", "95\\% CI upper"),
  booktabs = TRUE, 
  align = c("l","l","r","r","r"),
  escape = FALSE, 
  digits = 3,
  caption = "BCa 95\\% confidence intervals for annualised variance in the six-month periods with minimum and maximum variance."
) |>
  kable_styling(full_width = FALSE, position = "center")
```

<!-- ### Commenting on overlap -->

<!-- ### Instructions (to be removed later) -->

<!-- _Use the same subsamples as for Element 3. Use a suitable illustration to -->
<!-- compare the spread of your subsamples. Identify the six-month period with the -->
<!-- largest sample variance and the six-month period with the smallest sample -->
<!-- variance. Use bootstrapping/resampling to construct a 95% confidence interval -->
<!-- for_ $\sigma^2$ for each of these two subsamples separately. Comment on any -->
<!-- perceived overlap between these intervals. -->

### Analysis... (!change)

!BE note: scaling 6-month volatility/variance estimates to an annual basis so 
the numbers are easier to interpret and comparable (~126 trading days per 6M).
Have assumed the following:

$$
\hat{\mu}_{\text{annual}} = \hat{\mu}_{\text{daily}} \times 252
$$
$$
\hat{\sigma}_{\text{annual}}^2 = \hat{\sigma}_{\text{daily}}^2 \times 252
$$
$$
\hat{\sigma}_{\text{annual}} = \hat{\sigma}_{\text{daily}} \times \sqrt{252}
$$

The six-month period with the largest sample variance is
`r names(z_list)[min_var_loc]` and the six-month period with the smallest sample
variance is `r names(z_list)[max_var_loc]`.

!Paragraph here describing the method that we used to address each point we are
asked about

The 95% bias-corrected and accelerated (BCa) confidence intervals (CI) for the
variance ($\sigma^2$) for each of these two subsamples are shown in Table
\@ref(tab:var-ci). There is no overlap between the 95% CI for the 6-month ranges
with the minimum and maximum variance
[`r formatC(var_ci_min_bca[1], format = "fg", digits = 4)`,
`r formatC(var_ci_min_bca[2], format = "fg", digits = 4)`] and
[`r formatC(var_ci_max_bca[1], format = "fg", digits = 4)`,
`r formatC(var_ci_max_bca[2], format = "fg", digits = 4)`], respectively. Or in
scientific notation the 95% CI for the minimum variance subsample is
[`r formatC(var_ci_min_bca[1], format = "e", digits = 2)`,
`r formatC(var_ci_min_bca[2], format = "e", digits = 2)`] and the 95% CI for the
maximum variance subsample is
[`r formatC(var_ci_max_bca[1], format = "E", digits = 2)`,
`r formatC(var_ci_max_bca[2], format = "E", digits = 2)`].

This finding provides statistical evidence that the true variance $\sigma^2$ is
not the same in both periods. We have evidence therefore to reject the
assumption of constant variance in these data.

!BE note: check to see if can make this statement about CI's NOT overlapping.
Know that have to be careful about the statements made here.

<!-- $\big[`r formatC(var_ci_min_bca[1], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`, -->

<!-- `r formatC(var_ci_min_bca[2], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`\big]$ -->

<!-- and -->

<!-- $\big[`r formatC(var_ci_max_bca[1], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`, -->

<!-- `r formatC(var_ci_max_bca[2], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`\big]$, -->

<!-- respectively. -->

```{=latex}
\clearpage
\newpage
```

## Independence of increments {#elementFiv}

<!-- ### Instructions (to be removed later) -->

<!-- _The Efficient Markets Hypothesis states that the price of an asset today -->
<!-- incorporates all the information which is available about the asset, and -->
<!-- therefore that any changes to the price between today and tomorrow will be based -->
<!-- on new, and unpredictable, information which arrives in the meantime. The -->
<!-- practical result of this is that_ $z_{n}$ _should be independent of both_ -->
<!-- $z_{n−1}$ and $y_{n−1}$ _if the EMH is true._ -->

<!-- _Test the independence of_ ${z_{n}}$ _and_ ${z_{n-1}}$ _by using a contingency -->
<!-- table procedure. Let_ $q_1$, $q_2$, $q_3$ _represent the sample quartiles of -->
<!-- the_ $z$ _data. Classify each of the_ $z_n$ _observations according to whether -->
<!-- it is less than_ $q_1$_, between_ $q_1$ _and_ $q_2$_, between_ $q_2$ _and_ -->
<!-- $q_3$_, or greater than_ $q_3$_. Now draw up the contingency table to determine -->
<!-- whether the classification of_ $z_{n}$ _is dependent on the classiﬁcation of_ -->
<!-- $z_{n−1}$. -->

### Data preparation

Want to test fourth assumption - the $z_n$ increments are independent of each
other.

Test for dependence between $z_n$ and its immediate predecessor, $z_{n-1}$.

Test of the Efficient Markets Hypothesis, which implies that past returns
($z_{n-1}$) should not predict future returns ($z_n$)

<!-- # ```{r format-p-values} -->

<!-- # # BE note: can move this up, but have put here to discuss -->

<!-- # # want to ask Russell -->

<!-- # format.pval(one$p.value, eps = 0.0001, scientific = FALSE) -->

<!-- # ``` -->

```{r Element 5, echo=FALSE}
# ==============================================================================
#------------------- Element 5: independence of increments --------------------#
# ==============================================================================
```

```{r Element 5 pt1 - independence of increments, echo=FALSE}
# Create a $z_{n-1}$ series
z_lag <- stats::lag(z_n_clean, k = 1)

# Align $z_n$ and $z_{n-1}$ pairs by merging the xts objects
# inner join to remove the NA value created by the lag
z_merged <- merge(z_n_clean, z_lag, join = "inner")
colnames(z_merged) <- c("z_n", "z_n_minus_1")

# Categorise by quartile
# calculate quartiles from whole z_n_clean dataset
q <- quantile(z_n_clean, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

# create categorical variables
breaks  <- c(-Inf, q, Inf)
labels  <- c("Q1", "Q2", "Q3", "Q4")
z_n_cat <- cut(z_merged$z_n, breaks = breaks, labels = labels)
z_n_minus_1_cat <- cut(z_merged$z_n_minus_1, breaks = breaks, labels = labels)

# contingency
# rows $z_{n-1}$, columns $z_n$
con_table <- table(z_n_minus_1_cat, z_n_cat)

# statistical test (want Chi-squared, but need to see if cell frequencies are >5)
chisq_result <- chisq.test(con_table)
# chisq_result
# chisq_result$expected

# If any expected counts are low (R will often produce a warning), 
# the $\chi^2$ p-value is unreliable may then want to use Fisher's Exact Test
if (any(chisq_result$expected < 5)) {
  fisher_result <- fisher.test(con_table)
  fisher_result
}
```

```{r contingency-table, echo=FALSE, message=FALSE, warning=FALSE}
con_table_df <- as.data.frame.matrix(con_table)

con_table_df <- tibble::tibble(
  `$z_{n-1}$ quartile` = rownames(con_table),
  con_table_df
)

kable(
  con_table_df,
  caption = "Contingency table of quartile classifications for lagged log-returns $z_{n-1}$ (rows) and current log-returns $z_n$ (columns).",
  booktabs = TRUE,
  align = c("l", rep("r", 4)),
  escape = FALSE
) |>
  add_header_above(
    c(" " = 1, "$z_n$ quartile" = 4),
    escape = FALSE
  ) |>
  kable_styling(full_width = FALSE, position = "center")
```

None of the cell frequencies are \<5 so we can use chi-squared test.

All expected cell frequencies exceed 5, so the assumptions for Pearson’s
chi-squared test of independence are satisfied.

!BE note: talk about robustitity of Pearson's chi-squared test... Is it fragile?

The null hypothesis ($H_0$) for both tests is that the row and column variables
are independent - here that the categorical distribution of the current
log-return $z_n$ is independent of the distribution of the lagged log-return
$z_{n-1}$.

Pearson’s chi-squared test of independence: The hypotheses are defined as:
$$
\begin{cases}
H_0: \text{Today's quartile is independent of yesterday's} \\ 
H_1: \text{Today's quartile is dependent on yesterday's} 
\end{cases}
$$

The test returns $\chi^2$(\text{df} = `r chisq_result$parameter`) =
`r formatC(unname(chisq_result$statistic), format = "f", digits = 2)` with !p
`r format.pval(chisq_result$p.value, eps = 0.0001, scientific = FALSE)`
(p=`r formatC(chisq_result$p.value, format = 'e', digits = 3)`), so we reject
$H_0$. This provides statistical evidence that $z_n$ is dependent on the
classification of $z_{n-1}$ and this finding does not not support the Efficient
Markets Hypothesis.

```{=latex}
\clearpage
\newpage
```

## General upwardness of trend {#elementSix}

<!-- ### Instructions (to be removed later) -->

<!-- _One aspect of the data which might reflect a general tendency for the index to -->
<!-- increase might be the proportion of days when the price increment is positive. -->
<!-- Test whether this proportion is greater than 0.5._ -->

<!-- _Another aspect is the persistence of trends. A “positive run” is a sequence of -->
<!-- consecutive days when_ $Z_i > 0$; a “negative run” is a sequence of consecutive -->
<!-- days when $Z_i < 0$. (Note that a run may only be one day in length, or may be -->
<!-- longer than one day.) Identify the runs in your data set and test whether -->
<!-- positive runs have longer mean duration than negative runs. -->

```{r Element 6, echo=FALSE}
# ==============================================================================
#------------------- Element 6: general upwardness of trend -------------------#
# ==============================================================================
```

```{r Element 6 - proportion of positive increments, echo=FALSE}
num_positive <- sum(z_n_clean > 0)
total_days <- length(z_n_clean)
# test using binomial (could also approximate, but Russell normally says
# 'why approximate when you can calculate' so opted for that here)
binom_res <- binom.test(num_positive, total_days, p = 0.5, alternative = "greater")

#------------------- Element 6: persistence of trends
# use run length encoding
# changing to using z_n_clean_numeric to avoid error
# 'rle(signs) : 'x' must be a vector of an atomic type'
signs <- sign(z_n_clean_numeric)
runs <- rle(signs)

# runs object contains $values (1 for positive, -1 for negative) and
# $lengths (the duration of each run)
pos_run_lengths <- runs$lengths[runs$values == 1]
neg_run_lengths <- runs$lengths[runs$values == -1]

mean_pos_run <- mean(pos_run_lengths)
mean_neg_run <- mean(neg_run_lengths)

# could use t-test, but unlikely these are normally distributed
# non-parametric alternative is Mann-Whitney test (/Wilcoxon Rank-Sum)
# -> want to compare distributions of two groups without assuming normality
# Onesided here I think

wilcox_res <- wilcox.test(pos_run_lengths, neg_run_lengths, alternative = "greater")
```

### Proportion of positive increments

The proportion of days with a positive log-return is
`r formatC(num_positive / total_days, format = "fg", digits = 4)`.

A one tailed binomial test was used to calculate the probability of this
occurring assuming a null hypothesis, $H_0$, of $p=0.5$ ($H_1: \; p>0.5$)

The hypotheses are defined as:
$$
\begin{cases}
H_0: \text{Probability of a positive return is } \le 0.5 & (p \le 0.5) \\
H_1: \text{Probability of a positive return is } > 0.5 & (p > 0.5)
\end{cases}
$$


The exact binomial test assumes:

- Each day is classified as a binary outcome (success = positive return, failure
  = non-positive return).
- The sequence of days can be treated as approximately independent Bernoulli
  trials.
- The success probability p is constant over the period.
- The number of trials is fixed in advance.

These conditions are met here, however for financial data the independence and
constant- p assumptions are only approximate (because of possible serial
dependence and time-varying volatility). The test is still a reasonable first
descriptive check for the 'upwardness' of the index.

The binomial test found p `r format_p_vals(binom_res$p.value)` so we reject
$H_0$ in favour of alternative hypothesis that the true proportion of days with
positive increments is greater than 0.5. This confirms a general upward bias.

### Persistence of trends

A second question is whether upward movements tend to persist longer than
downward movements. Test hypothesis that positive runs (consecutive $z_n > 0$)
have a longer mean duration than negative runs (consecutive $z_n < 0$)

The mean length of positive runs is
`r formatC(mean_pos_run, format = "fg", digits = 3)` days, compared with
`r formatC(mean_neg_run, format = "fg", digits = 3)` days for negative runs.

A one-sided Mann-Whitney test (or Wilcoxon Rank-Sum test) was used with the null
hypothesis, $H_0$, that the mean of positive and negative runs are equal and an
alternative hypothesis that the mean of positive runs is greater than negative
runs.

The Mann–Whitney / Wilcoxon rank-sum test assumes:

- Two independent samples (here, the set of positive-run lengths and the set of
  negative-run lengths).
- The response is ordinal (run lengths are counts of days).
- Under $H_0$ the two populations have the same distribution.


The Mann–Whitney / Wilcoxon hypotheses are defined as:
$$
\begin{cases}
H_0: \text{Run lengths have identical distributions}  \\
H_1: \text{Positive runs tend to be longer} 
\end{cases}
$$

<!-- Run lengths are discrete and may not be perfectly independent in a strict time-series sense, but the test is robust to moderate deviations from these assumptions. Because run lengths are non-negative integers and can be skewed, a non-parametric test is more appropriate than a two-sample  -->
<!-- t-test, which would require approximate normality of run lengths in each group. -->

Wilcoxon rank sum test with continuity correct found p
`r format_p_vals(wilcox_res$p.value)` so we have sufficient evidence to reject
the null hypothesis in favour of the alternative hypothesis (the positive runs
have a shift greater than 0).

```{=latex}
\newpage
```
# Conclusions

While the log-normal model was invalidated by the data's non-normality
(\S\@ref(elementOne) & \S\@ref(elementTwo)), non-constant variance
(\S\@ref(elementFou)), and lack of independence (\S\@ref(elementFiv)), our
analysis for \S\@ref(elementThr) did not find statistical evidence to reject the
assumption of a constant mean.

To be updated:

Based on the Kruskal-Wallis Test the mean log-return appears to be constant over
time, even as the other core assumptions are violated.

!BE note/summary:

- element 1 (\S\@ref(elementOne)): Anderson-Darling test provides strong
  evidence that the $z_n$ data are not normally distributed
- element 2 (\S\@ref(elementTwo)): bootstrapping with z-data and normal
  distribution also provides evidence against an assumption of normality
- element 3 (\S\@ref(elementThr)): Kruskal–Wallis test $\therefore$ we accept
  null hypothesis that measures of central tendancy of log-returns over
  six-month periods share the same distribution (/medians are the same?)
- element 4 (\S\@ref(elementFou)): 95% BCa confidence intervals do not overlap,
  providing statistical evidence that (true) variance is not the same in the
  minimum and maximum six-month periods of sample variable evaluated
- element 5 (\S\@ref(elementFiv)): Chi-squared test provided evidence that $z_n$
  and $z_{n-1}$ are not indendent. BUT haven't checked for $y_{n-1}$ as not sure
  what this is...
- element 6 (\S\@ref(elementSix)): Part 1: Binomial test revealed proportion of
  positive-return days is statistically greater than 50% (insert p \<0.001),
  confirming a general upward bias. Part 2: Wilcoxon rank sum test with
  continuity correction provided sufficient statistical evidence (p\<0.001) to
  reject null hypothesis in favour of alternative hypothesis that the positive
  runs have a greater mean/median/distribution that the negative runs.

Want to wrap these up into some sort of formal conclusion...

- Something to discuss


<!-- ```{=html} -->
<!-- # References -->
<!-- <div id="refs"></div> -->
<!-- ``` -->

<!-- ```{=latex} -->
<!-- \bibliography{references.bib} -->
<!-- ``` -->

```{=latex}
\newpage
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}
```

# (APPENDIX) Appendices {-} 

# Supplemental methodology {#apd-supplementary-methods}

## Data cleaning & outlier selection {#apd-outlier-selection}

```{r appendix, echo = FALSE}
# ==============================================================================
#----------------------- Appendix: determining outliers -----------------------#
# ==============================================================================
```

```{r appendix-outlier-methods, echo=FALSE}
# Concept: use full range of Nasdaq data (i.e., since the 1970s) then plot 
# volatility and identify threshold to use to select outlier regions from our 
# time period
threshold_annual_volitility <- 0.70

nasdaq_data_full <- data.frame(
  date = index(IXIC_full),
  price = as.numeric(Cl(IXIC_full))
) %>%
  arrange(date) %>%
  mutate(
    log_ret = c(NA, diff(log(price))),
    # Get annualised volatility (*252) for no. of trading days per year
    volitility_20d = runSD(log_ret, n = 20) * sqrt(252),
    volitility_60d = runSD(log_ret, n = 60) * sqrt(252)
  ) %>%
  filter(!is.na(volitility_20d)) %>%
  filter(!is.na(volitility_60d))

# Get percentiles for breaches of threshold defined above
vol_percentiles <- quantile(
  nasdaq_data_full$volitility_20d,
  probs = c(0.90, 0.95, 0.99, 0.995),
  na.rm = TRUE
)

# get dates where 20d vol > threshold
nasdaq_breaches_full <- nasdaq_data_full %>%
  filter(volitility_20d > threshold_annual_volitility) %>%
  select(date, volitility_20d)

# print("Major Crash Dates detected:")
# nasdaq_breaches_full
```

As noted in \S\@ref(elementOne), our criteria for outliers was based upon a
definition of legitimate market activity. Whilst the definition of legitimate
activity is debatable, we adopted a conservative approach, excluding only
systemic shocks that represent a fundamental breakdown in market mechanics. Such
events are expected to occur less than once per decade. To identify these 
periods a numerical volatility threshold was implemented.

### Numerical Justification using Volatility Thresholding

Using the full history of the Nasdaq Composite (1971–2024), the annualised
20-day rolling volatility was calculated to benchmark the 2018–2024 study period
against historical extremes (Figure \@ref(fig:nasdaqAllTimeVol)).

```{r nasdaqAllTimeVol, eval = TRUE, echo = FALSE, fig.height=4.4, fig.pos='!ht', fig.cap="Nasdaq Composite index price volatility from 1971-2024. The dotted line shows the volatility threshold applied to identify outliers and the red shaded region indicates the excluded window in 2018-2024 associated with the COVID-19 pandemic. Note that the volatility metric peaks after the excluded region due to the lag inherent in a trailing 20-day calculation."}
#----------- Appendix Figure: full nasdaq rolling 20day volatility ------------#
appendix_p1 <- ggplot(nasdaq_data_full, aes(x = date)) +
  geom_line(aes(y = volitility_20d, color = "20-Day"), size = 0.3) +
  # geom_line(aes(y = volitility_60d, color = "60-Day"), size = 0.6, alpha = 0.8) +
  geom_hline(yintercept = threshold_annual_volitility, linetype = "dashed", color = "black") +
  annotate(
    "text",
    x = as.Date("1975-01-01"),
    y = threshold_annual_volitility+0.1,
    label = paste0("Threshold (", threshold_annual_volitility, ")"),
    size = 3
  ) +
  scale_color_manual(values = c("20-Day" = "black", "60-Day" = "blue")) +
  labs(
    title = paste0(
      "Nasdaq Composite Volatility (", 
      trading_period_days, 
      "-day rolling window)"
      ),
    x = "Date",
    y = bquote( sigma[n] ~ "(" * .(trading_period_days) * "-day rolling)")
  ) +
  # not visible when included so commented out
  # outlier_shading +
  ylim(0.01, 0.99) +
  theme_minimal() +
  theme(legend.position = "none")

# plot 2 (zoomed)
appendix_p2 <- ggplot(
  filter(nasdaq_data_full, date >= "2018-01-01" & date <= "2025-12-31"),
  aes(x = date)
) +
  geom_line(aes(y = volitility_20d, color = "20-Day"), size = 0.5) +
  # geom_line(aes(y = volitility_60d, color = "60-Day"), size = 0.8, alpha = 0.8) +
  # covid shading
  outlier_shading +
  # annotate(
  #   "text",
  #   x = as.Date("2020-03-01"), y = 0.9,
  #   label = "COVID Exclusion",
  #   color = "black", size = 3, hjust = 0
  # ) +
  scale_color_manual(values = c("20-Day" = "black", "60-Day" = "blue")) +
  labs(
    title = paste0(
      "Nasdaq Composite Volatility (", 
      trading_period_days, 
      "-day rolling window)"
      ),
    x = "Date",
    y = bquote( sigma[n] ~ "(" * .(trading_period_days) * "-day rolling)")
  ) +
  ylim(0.01, 0.99) +
  # theme(legend.position = "none") +
  theme_minimal()

(appendix_p1 / appendix_p2) &
  theme(
    plot.title  = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5),
    legend.position = "none"
  )
```

A threshold of 0.70 (70% annualised volatility) was selected. Note that
annualised volatility can exceed 100%. This metric represents the standard
deviation of daily log-returns scaled to an annual timeframe 
$$
\sigma_{ann} = \sigma_{daily} \times \sqrt{252}
$$
As shown in Figure \@ref(fig:nasdaqAllTimeVol), breaches of the 0.70 level
isolate only five distinct 'systemic shock' events over the last 50 years:

1. Black Monday (1987)
2. The Dot-Com Bubble Burst (2000)
3. The 9/11 Recession (2001)
4. The Global Financial Crisis (2008)
5. The COVID-19 Pandemic (2020)

The five events that breach the 0.70 threshold correspond to the
`r formatC(vol_percentiles['99.5%'], format="fg", digits=3)`th 
percentile of all trading days since 1971. Other periods of interest within the
2018-2024 window, noted in Figure \@ref(fig:timeline-fig), such as the
"Christmas Eve Massacre" (2018), the Trade War shocks (2019), and the Inflation
Bear Market (2022), did not breach the 0.70 threshold and fall into the category
of legitimate and 'standard' market activity for the purposes of this report.

<!-- There is strong quantitative evidence to characterise the onset of the COVID-19 -->
<!-- pandemic as a non-representative outlier (or a period which falls outside of our -->
<!-- definition of 'legitimate market activity').  -->


### Technical boundary definition

While the volatility threshold flagged the 2020 COVID period as a systemic
anomaly, relying solely on a rolling metric can result in imprecise window
boundaries due to lag effects. To define the exact start and end dates of the
exclusion window, technical trend line analysis was applied to the daily price
action (Figure \@ref(fig:bloomberg-trends)).

The pre-pandemic market followed a consistent bullish trajectory, delineated by
the upward trend line LU1. The onset of the exclusion window was defined as
24 February 2020 ("Point A"). On this date, the closing price structurally
breached LU1 to the downside. This technical breakdown coincided with
significant geopolitical escalations, specifically the declaration of a national
lockdown in Italy and subsequent mitigating measures announced by the US Centers
for Disease Control and Prevention. Following the breach, the market entered a
phase of panic selling captured by the steep downward trend line LD1. This trend
encompassed the declaration of a US nationwide emergency on 13th March 2025. The
end of this volatility period was identified as 23 March 2020 ("Point B").
At this point, the closing price breached the downward trend line LD1 to the
upside. Post-breach analysis indicates a resumption of a stable bullish
trajectory (trend line LU2), suggesting a return to standard market mechanics.

The dates from 2018-2024 excluded for the analysis in this report were 
24 February 2020 to 23 March 2020.

```{r bloomberg-trends, echo=FALSE, out.width="100%", fig.cap="Technical trend analysis of the Nasdaq Composite index via the Bloomberg Terminal interface (Jul 2019–Aug 2021). The exclusion window is bounded by two structural breaks, the violation of the pre-pandemic uptrend (LU1) at Point A (Feb 24, 2020) and the conclusion of the panic-selling downtrend (LD1) at Point B (Mar 23, 2020), after which a recovery trend (LU2) is established."}
knitr::include_graphics("fig/appendix-COVID19-selection-3.pdf")
```

<!-- tools::showNonASCIIfile("your_file.Rmd") -->

## Statistical test selection {#apd-test-rationale}

### Test of normality (\S\@ref(elementOne)) 

Three tests for normality were considered, the ... 

Their use cases are outlined in the Table below (Table xx).

For this report, the data were found to be (put stats from section 1).  
Test AD was suitable. 
Therefore test 1 wasn't suitable because y and test 2 was not suitable because of z.

```{=latex}
\newpage
```

# Additional results

## Observations per Period 

In \S\@ref(elementThr) the log-returns data were split into six-month periods,
as shown in Figure \@ref(fig:element3-boxplot).

```{r, echo=FALSE, out.width="70%"}
# knitr::include_graphics("fig/element3-boxplot-1.pdf")
```



```{=latex}
\newpage
```

# Reproducibility, accessibility & declarations (Gen-AI & word count)
<!-- BE note: taken out as we are submitting R file separately -->
<!-- Use: `knitr::purl("cs1-group07.rmd", documentation = 0)` to generate r file -->
<!-- Use: `tools::showNonASCIIfile("cs1-group07.rmd")` to debug -->
<!-- ## R Code Documentation -->
<!-- ```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
``` -->

## Reproducibility & accessibility 

An accessible HTML version of this report is available via a public GitHub page:

- https://ytterbiu.github.io/a_group07-project

This report was created in R Markdown. The source code is open source and is
available via:

- https://github.com/ytterbiu/a_group07-project/tree/main. 

Changes made to this document were tracked using Git and are also available via
the same repository
(https://github.com/ytterbiu/a_group07-project/commits/main/). Please note that
this commit history reflects updates made to the project repository and is not
a comprehensive timeline of all contributions by group members.

## Generative AI Declaration

Generative AI (GenAI) tools were used to assist with project planning,
methodological validation, and R debugging throughout this project. The GenAI
tools served as an additional group member in this capacity.

### Tools Used

Google Gemini 2.5 Pro (Deep Research), ChatGPT 5

### Prompts used

The GenAI tools were provided with the 'Coursework Group 7.pdf' and extracts
from our R code.

A non-exhaustive list of prompts used is shown below. We opted to include at
least one example of each _type_ of prompt used, rather than a list of all
prompts. For example, if we entered the prompts 'How can we put two ggplots side
by side?' and 'How to remove grey background on a ggplot', we would include only
the first of these in the list below, as both relate to plotting issues (& are
similar to 'querying' with a traditional search engine).

- "Can you help with outlining the steps for this project."
- "...the top of both graphs in r are slightly different. (Pasting R code.) Can
  this be fixed for the plot area?"
- "Help phrasing this in a simple and accessible way - remove any jargon &
  identify the key points"
- "How can this error be tracked down in an rmd file? ! Text line contains an
  invalid character. l.1 ^^@^^@..."
- ...

### Use of GenAI outputs & subsequent revisions

The GenAI tools served as a additional group member. Notably, the GenAI output:

- Helped debug plotting issues.
- Raised potential statistical traps (e.g., the "ANOVA trap" and the "data
  cleansing order" trap).
- Suggested appropriate non-parametric tests (Kruskal-Wallis, Wilcoxon Rank-Sum)
  where the assumptions of parametric tests (ANOVA, t-test) were violated.
- Recommended specific R packages (nortest, e1071) and functions (ad.test,
  kruskal.test) best suited for each task.
- Commented on the structure of R code snippets and the report structure.

### Changes made:

The AI's suggestions were critically reviewed and debated by the group. Plotting
formatting in ggplot, ... (add others) and... were all implemented following an
iterative feedback cycle.

The R code for implementation was written and debugged by the group. GenAI tools
were used, and thought of, in the capacity of an additional group member. All
final analysis, interpretation, and writing in this report were generated by
(human) group members.

## Word count

The R Markdown file was used to produce a `tex` file. The tool `texcount` was 
then used to count the words within each section. The output of `texcount` is 
below:

```bash
$ texcount cs1-group07.tex      

File: cs1-group07.tex
Encoding: utf8
Words in text: 2801
Words in headers: 139
Words outside text (captions, etc.): 419
Number of headers: 37
Number of floats/tables/figures: 14
Number of math inlines: 53
Number of math displayed: 10
Subcounts:
  text+headers+captions (#headers/#floats/#inlines/#displayed)
  4+8+0 (1/0/0/0) _top_
  0+1+0 (1/0/0/0) Section: Introduction}
  4+2+0 (1/0/0/0) Subsection: Executive summary}
  137+1+49 (1/0/4/1) Subsection: Background}
  0+2+0 (1/0/0/0) Section: Individual elements}
  174+7+152 (1/3/3/1) Subsection: Data cleaning and standard test of normality}
  316+31+38 (3/2/5/0) Subsection: Investigation of normality by resampling}
  228+10+21 (3/2/6/2) Subsection: Investigation of constant mean}
  200+6+89 (2/4/7/3) Subsection: Investigation of constant variance}
  157+5+13 (2/1/14/1) Subsection: Independence of increments}
  323+11+0 (3/0/8/2) Subsection: General upwardness of trend}
  244+1+0 (1/0/5/0) Section: Conclusions}
...
```

These correspond to a total word count of !x for the executive summary, 
introduction, elements 1-6, and the conclusion.

---

[^log-symmetry]:
    With arithmetic returns, a 100% gain (+100%) is cancelled out by a 50% loss,
    creating an asymmetry that complicates statistical aggregation. Log-returns
    resolve this as a move from $100 to $200 (log-return of +0.693) is the exact
    additive inverse of a move from $200 to $100 ( log-return of −0.693).

[^cont2001quote]:
    A 'stylized empirical fact' is defined by @Cont01022001 as being
    "properties common across a wide range of instruments, markets and time
    periods" [@Cont01022001 pp. 224]

[^legit-market]:
    We acknowledge that what constitutes “legitimate” market activity is itself
    debatable. This is discussed further in Appendix 
    \S\@ref(apd-outlier-selection).

