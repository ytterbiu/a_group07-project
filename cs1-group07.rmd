---
title: "SMM047 Group Coursework 2025-26"
subtitle: "Group 07"
author:
  - "Abdulrahman Alolyan"
  - "Benjamin Evans"
  - "Amogh Sharma"
  - "Nivetha Subbiah"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  bookdown::html_document2:
    self_contained: true
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
    df_print: paged
  # runtime: shiny
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    number_sections: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: true
# fontsize: 10pt
bibliography: references.bib
---

```{=latex}
\newpage
```

# Introduction

## Executive summary

Summary / Abstract here

Key points (BE clarified with Russell)

- Write for actuarial supervisor (boss, but who understands statistics)

- Give p-values to 2dp if \>0.01, 3dp if \<0.01 and if below 0.001 then write
  \<0.001. (BE note: make custom function for in text R functionality)

- These assumptions are used because they are mathematically convenient, not
  necessarily because they are true... This is known

- Don't have to include a load of references, but no harm

- For COVID data need to include data driven justification for the range of
  dates chosen... This is a good point to include a reference or some kind of
  metric

- For each statistical test used, Russell wants to see that we know the
  conditions for its use (where it can and can't be used and why it's
  appropriate for use in the case we are discussing).

- Gamma 1 is the skewness and gamma 2 is the kurtosis so double gamma 1's are
  appropriate in element 2. If wanting to label separately then would use gamma
  1 x and gamma 1 y (etc.)

```{=html}
<!-- ## Key Information

### Background

This R Markdown document was created as part of a group assignment for SMM047 at Bayes Business School, City St George's, University of London in Term 1 2025-26.

-   *BE note: use `rmarkdown::render("cs1-group07.rmd", output_format = "all")` to render this document* -->
```

```{r setup-knitr, include=FALSE, echo=FALSE}
#----------------------- Initial setup (knitr settings) -----------------------#
dir.create("fig", showWarnings = FALSE)

# Defaults common to all outputs
knitr::opts_chunk$set(
  echo     = TRUE,
  message  = FALSE,
  warning  = FALSE,
  fig.align = "center",
  out.width = "100%",
  fig.path  = "fig/",
  dpi       = 300
)

# Output-specific settings
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "pdf",
    fig.pos = "ht",
    out.extra = ""
  )
} else {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "svglite"  # or "png"
  )
}
```

```{r setup-qol, include=FALSE, echo=FALSE}
#----------------------------- Clean environment ------------------------------#
rm(list = ls()) # Remove all objects
graphics.off() # Close all graphical devices
cat("\014") # Clean console
```

```{r load-dependencies, include=FALSE, echo=FALSE}
#------------------- Load dependencies / external libraries -------------------#
library(quantmod)
library(xts) # for downloading
library(zoo) # for downloading

library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
library(kableExtra)

library(e1071) # statistical tests
library(summarytools) # statistical tests (but not used other than demo)
library(nortest) # normality test

library(boot) # bootstrapping
```

```{r custom-functions, include=FALSE, echo=FALSE}
#---------------------------- Custom QOL functions ----------------------------#
# function: banner comments (used to to section up code)
# Usage: cat(banner_comment("Element 1: data cleaning"), "\n")
banner_comment <- function(text, width = 80,
                           border = "#", fill = "-") {
  txt <- paste0(" ", text, " ")
  inner_width <- width - 2 * nchar(border)

  if (inner_width <= nchar(txt)) {
    return(paste0(border, txt, border))
  }

  pad_total <- inner_width - nchar(txt)
  pad_left  <- pad_total %/% 2
  pad_right <- pad_total - pad_left

  paste0(
    border,
    strrep(fill, pad_left),
    txt,
    strrep(fill, pad_right),
    border
  )
}
# function: format p-values for text
format_p_vals <- function(p) {
  if (length(p) != 1L || is.na(p)) {
    stop("Error! p must be a single non-missing value")
  }
  if (p > 1) {
    stop("Error! Value greater than 1")
  }
  if (p < 0) {
    stop("Error! Value less than 0")
  }

  if (p >= 0.01) {
    paste0("= ", formatC(p, format = "f", digits = 2))
  } else if (p >= 0.001) {
    paste0("= ", formatC(p, format = "f", digits = 3))
  } else {
    "< 0.001"
  }
}
```

```{r Download-data, include=FALSE}
#---------------------------- Download / Load data ----------------------------#
# Define inputs & date range
ticker <- "^IXIC"
from   <- as.Date("2018-01-01")
to     <- as.Date("2024-12-31")

# Clean ticker for filename (remove ^ and any non-alphanumerics)
ticker_clean <- gsub("[^A-Za-z0-9]", "", ticker)

# Create filename (IXIC20180101to20241231.csv)
fname <- sprintf(
  "%s%sto%s.csv",
  ticker_clean,
  format(from, "%Y%m%d"),
  format(to,   "%Y%m%d")
)

load_from_csv <- function(path) {
  dat <- read.csv(path, stringsAsFactors = FALSE)
  # req date -> if missing force error & re-download
  if (!"Date" %in% names(dat)) {
    stop("CSV does not have expected 'Date' column.")
  }
  xts::xts(
    dat[ , setdiff(names(dat), "Date"), drop = FALSE],
    order.by = as.Date(dat$Date)
  )
}

# Download data from our given ticker
download_ixic <- function(ticker, from, to) {
  getSymbols(
    ticker,
    from        = from,
    to          = to,
    auto.assign = FALSE
  )
}

use_download <- FALSE
# Check to see if file exists -> if not then download data
if (file.exists(fname)) {
  IXIC <- tryCatch(
    load_from_csv(fname),
    error = function(e) {
      message("Cached file invalid, downloading fresh data: ", e$message)
      use_download <<- TRUE
      download_ixic(ticker, from, to)
    }
  )
} else {
  use_download <- TRUE
  IXIC <- download_ixic(ticker, from, to)
}

# If downloaded write/rewrite clean csv
if (use_download) {
  ixic_df <- data.frame(
    Date = index(IXIC),
    coredata(IXIC)
  )
  write.csv(ixic_df, fname, row.names = FALSE)
}

# (old) Download the data
# getSymbols("^IXIC", from = "2018-01-01", to = "2024-12-31")
```

```{r Prepare data, echo=FALSE}
# First we prepare our data, getting the adjusted close price, applying a log
# transform, calculating the log-return increment, and removing NaN values.

# get adjusted close price
IXIC_ad <- Ad(IXIC)
# log transform
y_n <- log(IXIC_ad)
# log-return increment (z_n) - get difference in log transformed data
z_n <- diff(y_n)
# remove NA values
z_n <- na.omit(z_n)
```

## Background

!BE note: want background introductory paragraph here

- https://indexes.nasdaq.com/Index/Overview/COMP
- https://www.nasdaq.com/solutions/global-indexes/nasdaq-composite
- (from above: https://indexes.nasdaqomx.com/docs/FS_COMP.pdf
  https://indexes.nasdaqomx.com/docs/methodology_comp.pdf)

This report analyses the Nasdaq Composite Index (`^IXIC`) using data from
January 1, 2018, to December 31, 2024. The Nasdaq Composite Index a stock market
index that includes thousands of stocks listed on the Nasdaq exchange. It's
heavily focused on technology companies and is widely used as a key benchmark
for the tech sector's performance. As a result, this index's behaviour can
impact many financial products (including index-tracking funds).

The adjusted closing price was analysed because, unlike the regular closing
price, it accounts for corporate actions like dividends and stock splits. This
gives a much more accurate picture of an investor's actual returns.

The log-returns were calculated from the raw US dollar prices. This metric (BE
note: is metric the correct word?) measures the daily change in a way that is
time-additive (meaning daily log-returns sum up to the total log-return for the
week) and mathematically symmetric (!refs)[^log-symmetry]. This symmetry makes
log-returns a more suitable method of evaluating an index's volatility and
day-to-day movements. The data were cleaned and any missing values were removed
(further data cleaning is described in \S\@ref(elementOne)). The historical
price of the index and its daily log-returns for 2018-2024 are shown in Figure
\@ref(fig:initial-visualisation).

<!-- Daily adjusted closing values for ^IXIC over the period 1 January 2018 to 31 -->
<!-- December 2024 were extracted from ... using `getSymbols` from the `quantmod` -->
<!-- package. These data were then used as the basis for our statistical evaluation -->
<!-- in this report. Used adjusted closing because -->
<!-- First we can prepare our data, getting the adjusted close price, applying a log -->
<!-- transform, calculating the log-return increment, and removing NaN values. -->
<!-- The Nasdaq Composite Index over the period 1 January 2018 to 31 December 2024, -->
<!-- and the log-return increment are shown in \@ref(fig:initial-visualisation). -->

```{r initial-visualisation, fig.cap = "Nasdaq Composite price trend and daily volatility (2018–2024). The left figure shows the daily adjusted closing price. The right figure shows the daily log-returns, highlighting day-to-day percentage changes and volitility.", echo = FALSE}
# same length as IXIC_ad, first value NA
z_n_xts <- diff(log(IXIC_ad))

df_initial_ixic <- tibble(
  date = index(IXIC_ad),
  price = as.numeric(IXIC_ad),
  z_n = as.numeric(z_n_xts)
) %>%
  drop_na(z_n)

date_range <- range(df_initial_ixic$date, na.rm = TRUE)

p1_initial_vis <- ggplot(df_initial_ixic, aes(date, price)) +
  theme_bw() +
  geom_line(linewidth = 0.5) +
  labs(
    title = "NASDAQ Composite\n (Adjusted Close)",
    x = "Date",
    y = "Adjusted
Close"
  )

p2_initial_vis <- ggplot(df_initial_ixic, aes(date, z_n)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  labs(
    title = expression(Log ~ Return ~ Increment ~ (z[n])),
    x = "Date",
    y = expression(z[n])
  )

# gridExtra::grid.arrange(p1, p2, ncol = 2)
(p1_initial_vis | p2_initial_vis) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )
```

# Individual elements

## Data cleaning and standard test of normality {#elementOne}

### Identifying and removing outliers

A review of the Nasdaq Composite adjusted closing price over 2018-2024 was
undertaken to identify and extract the dates of periods of elevated volatility.
Potential outliers were considered for exclusion if the volatility did not arise
from legitimate market activity. A timeline showing regions of volatility and
their primary attributable causes are shown in Figure !x. Following numerical
analysis (!describe) and group discussion, only one period associated with onset
of the COVID-19 pandemic was selected for exclusion. The data driven procedure
used to define the COVID-19 exclusion window is described in more detail in the
appendix (\S\@ref(apd-COVID19-selection)).

We acknowledge that what constitutes “legitimate” market activity is itself
debatable. One could potentially exclude other volatile periods, but doing so
would not align with the aim of this analysis, which is to test how well the
standard log-return model describes the behaviour of the Nasdaq Composite under
realistic market conditions.

<!-- It is suggested that we remove data from the 2020/21 COVID-19 period -->
<!-- ($log~x_{20May2020}$ and $log~x_{09Apr2020}$), when markets were heavily -->
<!-- disrupted. The extreme, policy-driven movements in this window can be argued to -->
<!-- not reflect the “normal” behaviour of the IXIC index. -->

<!-- As noted in the assessment instructions, if we delete dates from the dataset, we -->
<!-- cannot then treat the jump between the last date before the gap and the first -->
<!-- date after the gap as a valid increment ($z_n$). Calculating log-returns first -->
<!-- and only afterwards drop rows for the excluded period, our data would contain -->
<!-- incorrect 'links'. -->

<!-- Cleaning has to happen before we compute any returns. -->

```{r Element 1 - cleaning}
#---------- Element 1: data cleaning and standard test of normality -----------#

# outlier_periods_to_remove <- c(
#   "2020-03-01/2020-04-17",
#   "2021-01-10/2021-03-20",
#   "2022-06-01/2022-07-05"
# )
outlier_periods_to_remove <- c(
  "2020-02-24/2020-03-23"
)
# set all values from outlier period to NA
y_n_clean <- y_n
for (p in outlier_periods_to_remove) {
  y_n_clean[p] <- NA
}
# calculate log-return increments
z_n_clean <- diff(y_n_clean)
z_n_clean_pre_na_drop <- z_n_clean
# remove NA increments
z_n_clean <- na.omit(z_n_clean)
```

New visualisation to show where the periods to remove are... We will probably
also want to put these in a table? Or could describe in text - don't feel
strongly either way.

!BE note: having made Figure \@ref(fig:regions-to-be-removed), think this figure
combined with an intext description will be best.

```{r regions-to-be-removed, echo = FALSE, fig.cap = "Initial visualisation and regions selected for removal"}
outlier_df <- tibble(
  id     = seq_along(outlier_periods_to_remove),
  period = outlier_periods_to_remove
) %>%
  tidyr::separate(period, into = c("start", "end"), sep = "/", convert = TRUE) %>%
  mutate(
    start = as.Date(start),
    end   = as.Date(end),
    mid   = start + floor(as.numeric(end - start) / 2),
    label = as.character(id)
  )

z_n_xts <- diff(log(IXIC_ad))

df <- tibble(
  date  = index(IXIC_ad),
  price = as.numeric(IXIC_ad),
  z_n   = as.numeric(z_n_xts)
) %>%
  drop_na(z_n)

range_p1 <- range(df$price, na.rm = TRUE)
range_p2 <- range(df$z_n,   na.rm = TRUE)

label_y_p1 <- range_p1[2] - 0.08 * diff(range_p1)
label_y_p2 <- range_p2[2] - 0.02 * diff(range_p2)
# label_y_p1 <- label_y_p1*1.02
# label_y_p2 <- label_y_p2*1.2

base_theme <- theme_bw() +
  theme(
    plot.title  = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

p1 <- ggplot(df, aes(date, price)) +
  geom_line() +
  labs(
    title = "NASDAQ Composite\n(Adjusted Close)",
    x = "Date", y = "Adjusted Close"
  ) +
  base_theme +
  # vertical dotted lines
  geom_vline(
    data = outlier_df,
    aes(xintercept = start),
    linetype = "dotted",
    inherit.aes = FALSE
  ) +
  geom_vline(
    data = outlier_df,
    aes(xintercept = end),
    linetype = "dotted",
    inherit.aes = FALSE
  ) +
  # circles
  geom_point(
    data = outlier_df,
    aes(x = mid, y = label_y_p1),
    inherit.aes = FALSE,
    shape = 21,
    fill  = "white",
    color = "black",
    size  = 6,
    stroke = 0.6
  ) +
  # numbers
  geom_text(
    data = outlier_df,
    aes(x = mid, y = label_y_p1, label = label),
    inherit.aes = FALSE,
    vjust = 0.35,
    size = 3
  ) #+
  #scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)))

p2 <- ggplot(df, aes(date, z_n)) +
  geom_line() +
  labs(
    title = expression(Log~Return~Increment~(z[n])),
    x = "Date",
    y = expression(z[n])
  ) +
  base_theme +
  geom_vline(
    data = outlier_df,
    aes(xintercept = start),
    linetype = "dotted",
    inherit.aes = FALSE
  ) +
  geom_vline(
    data = outlier_df,
    aes(xintercept = end),
    linetype = "dotted",
    inherit.aes = FALSE
  ) #+
  # geom_point(
  #   data = outlier_df,
  #   aes(x = mid, y = label_y_p2),
  #   inherit.aes = FALSE,
  #   shape = 21,
  #   fill  = "white",
  #   color = "black",
  #   size  = 6,
  #   stroke = 0.6
  # ) +
  # geom_text(
  #   data = outlier_df,
  #   aes(x = mid, y = label_y_p2, label = label),
  #   inherit.aes = FALSE,
  #   vjust = 0.35,
  #   size = 3
  # ) #+
  #scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)))

(p1 / p2)
```

Quick visualisation to compare new vs old:

```{r raw-vs-clean, echo=FALSE, fig.cap = "Raw vs clean* TBC"}
df_raw <- tibble(
  date = index(z_n),
  z_n  = as.numeric(z_n)
)
df_clean <- tibble(
  date = index(z_n_clean_pre_na_drop),
  z_n  = as.numeric(z_n_clean_pre_na_drop)
)
ylim_all <- range(df_raw$z_n, df_clean$z_n, na.rm = TRUE)


p_raw <- ggplot(df_raw, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(pre exclusion)",
    x = "Date",
    y = expression(z[n])
  )

p_clean <- ggplot(df_clean, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(post exclusion)",
    x = "Date",
    y = expression(z[n])
  )

(p_raw | p_clean) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

```

### Sample statistics

<!-- BE note: Skewness and excess kurtosis are computed using e1071 with type = 1. -->

Skewness and excess kurtosis of the cleaned log-returns $z_n$ were computed
using the `e1071` package in R with type = 1 [@R-e1071]. This corresponds to the
classical moment-based definitions $m_3/m_2^{3/2}$ and $m_4/m_2{^2} - 3$
described by @joanes1998, and used in SMM047 2025-26
[@gerrard_seg2_notes_questions]. These measures are widely used in textbooks and
provide a direct description of the shape of the empirical distribution.

Alternative software implementations, such as the package `summarytools`, were
considered. These packages apply small-sample bias corrections (“type 3”
estimators), which lead to slightly different values but are asymptotically
equivalent. Given our relatively large sample size and descriptive aim, the type
1 definitions are appropriate and fully adequate.

```{r Element 1 pt2}
# e1071::skewness(z_n_clean, type = 1)

z_n_clean_numeric <- as.numeric(z_n_clean)

desc <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness (Type 1)",
    "Excess Kurtosis (Type 1)"
  ),
  Value = c(
    mean(z_n_clean_numeric),
    var(z_n_clean_numeric),
    e1071::skewness(z_n_clean_numeric, type = 1),
    # already excess kurtosis for type = 1
    e1071::kurtosis(z_n_clean_numeric, type = 1)
  )
) |>
  mutate(Value = sprintf("%.6f", Value))
```

```{r zn-desc, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  desc,
  caption = "Descriptive statistics of cleaned log returns ($z_n$).",
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")
```

BE note: I have included a quick demo of this here:

```{r Element 1 pt3, echo=FALSE}
# e1071::skewness(z_n_clean, type = 1)

z_n_clean_numeric <- as.numeric(z_n_clean)

desc_ad <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness (Type 1)",
    "Skewness (Type 2)",
    "Skewness (Type 3)",
    "Excess Kurtosis (Type 1)",
    "Excess Kurtosis (Type 2)",
    "Excess Kurtosis (Type 3)"
  ),
  Value = c(
    mean(z_n_clean_numeric),
    var(z_n_clean_numeric),
    e1071::skewness(z_n_clean_numeric, type = 1),
    e1071::skewness(z_n_clean_numeric, type = 2),
    e1071::skewness(z_n_clean_numeric, type = 3),
    # already excess kurtosis for type = 1
    e1071::kurtosis(z_n_clean_numeric, type = 1),
    e1071::kurtosis(z_n_clean_numeric, type = 2),
    e1071::kurtosis(z_n_clean_numeric, type = 3)
  )
) |>
  mutate(Value = sprintf("%.6f", Value))
```

```{r zn-desc-ad, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  desc_ad,
  caption = "Additional descriptive statistics of cleaned log returns ($z_n$).",
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")
```

```{r Element 1 old summary tools, results = "asis", echo=FALSE}
#------------------- summarytools demo
summary_table <- descr(
  z_n_clean,
  stats = c("mean", "sd", "skewness", "kurtosis"),
  transpose = TRUE,
  headings = FALSE,
  round.digits = 6,
  plain.ascii = FALSE,
  style = "rmarkdown"
)
```

Output of `summarytools` (the technique introduced in the lab session).
Summarytools does not output the variance, but we can calculate by squaring the
standard deviation. Output from `summarytools` for variance is
`r formatC(summary_table$Std.Dev^2, format = "fg", digits = 4)` , compared to
`r formatC(var(z_n_clean_numeric), format = "fg", digits = 4)` from `e1071`.

```{r, results = "asis", echo=FALSE}
print(summary_table)
# summary_table$Std.Dev^2
# boxplot(z_n)


# # COVID: 10/04/2020-19/05/2020
#
# IXIC_ad <- IXIC_ad[
#   !((index(IXIC_ad) >= "2020-04-10") &
#     (index(IXIC_ad) <= "2020-05-19"))
# ]
#
#
# z_n = na.omit(log(IXIC_ad / dplyr::lag(IXIC_ad)))

# plot(z_n)
# boxplot(z_n)
```

_Note how the skewness and kurtosis from `summarytools` match the type 3
skewness and kurtosis from `e1071`._

### Test of normality {#elementOneAdtest}

```{=html}
<!-- Given that financial log-returns are widely theorised to be leptokurtic
("fat-tailed"), the Anderson-Darling test is a suitable test for detecting the
exact type of non-normality that is expected. -->
```

```{r Test of normality}
ad_test_result <- ad.test(z_n_clean)
ad_test_result
```

!BE note: background on Anderson-Darling test
https://www.6sigma.us/six-sigma-in-focus/anderson-darling-normality-test/

Since the 1960's studies have found that asset returns (in particular, daily
log-returns) are leptokurtic, with a higher peak and heavier tails than the
normal distribution [@Mandelbrot1963; @Fama1965]. So much so that in 2001 the
heavy tails of financial were identified as being a "stylized empirical fact" by
@Cont01022001[^cont2001quote].

The Anderson-Darling test is an appropriate test for these data. It

- List properties...
- Include reference

was selected to test the normality of these data as is it more sensitive to
deviations in the tails of the distribution.

The null hypothesis ($H_0$) for the Anderson-Darling test is that the data are
normally distributed.

Normality was assessed using the Anderson-Darling test, with a test statistic of
A = `r round(ad_test_result$statistic, 1)` (p
`r format_p_vals(ad_test_result$p.value)`). We therefore reject the null
hypothesis that the data are normally distributed ($H_0$), in favour of the
alternative hypothesis $H_1$. The Anderson-Darling test provides strong evidence
that the $z_n$ data are not normally distributed.

## Investigation of normality by resampling {#elementTwo}

### Instructions (to be removed later)

_You are going to investigate whether the difference between your sample and a
sample from a normal distribution becomes apparent when constructing confidence
intervals for sample skewness._

_(a) Simulate a sample of size 13 from a Normal distribution with mean and
variance equal to the sample mean and variance you have just calculated.
Calculate the sample skewness from this simulated sample. Repeat this until you
have generated 50,000 values of_ $\hat{\gamma}_1$.

_(b) Use resampling to obtain a sample of size 13 from your z-data. Calculate
the sample skewness from this bootstrap sample. Repeat this until you have
generated 50,000 values of_ $\hat{\gamma}_1$.

_(c) Use suitable illustrations to investigate the differences between the two
sets of_ $\hat{\gamma}$ values. Comment on what you observe and whether the
outcome of your tests confirms the result of the test of normality.

### Simulating skewness from resampling a normal distribution & from $z$-data using bootstrapping

Idea: compare the sampling distribution of skewness from the data against the
sampling distribution of skewness from a true normal distribution, using the
parameters from IXIC data.

We are told to use a small n ($n=13$) so would expect non-normal despite
underlying data being normal.

Idea: then repeat this process using bootstrapping and z-data.

Non-parametric bootstrap...

```{r Element 2 pt1 - Simulating skewness from a normal distribution, cache = FALSE}
#------------ Element 2: investigation of normality by resampling -------------#
# set random seed
set.seed(1234)

# set parameters from z_data (mean & st-dev)
z_mean <- mean(z_n_clean)
z_sd <- sd(z_n_clean)
N_reps <- 50000
n_sample_size <- 13

# Simulate 50,000 skewness values using sample size of 13
skew_normal_sim <- replicate(
  N_reps, e1071::skewness(rnorm(n_sample_size, z_mean, z_sd), type = 1)
  )

# bootstrapping skewness simulation
skew_bootstrap <- replicate(
  N_reps, e1071::skewness(sample(z_n_clean, n_sample_size, replace = TRUE), type = 1)
  )

#------------ basic
skew_normal_sim_median <- median(skew_normal_sim)
skew_normal_bootstrap  <- median(skew_bootstrap)
sd(skew_normal_sim)
sd(skew_bootstrap)
el2_original_skew <- e1071::skewness(z_n_clean, type = 1)
el2_bias <- mean(skew_bootstrap) - el2_original_skew

#------------ statistics/metrics
# calculating quantile based confidence interval
print(quantile(skew_bootstrap, probs = c(0.025, 0.975)))
print(quantile(skew_normal_sim, probs = c(0.025, 0.975)))
```

```{r el2-testing, echo=FALSE, cache = FALSE}

# Using open source: https://gitlab.com/scottkosty/bootstrap/-/blob/master/R/bcanon.R
#
# Takes a vector of observations x, the number of bootstrap samples to take,
# an estimator plus additional parameters for it, and confidence levels for
# the output intervals
"bcanon" <- function(x,nboot,theta,...,alpha =
                     c(.025,.05,.1,.16,.84,.9,.95,.975)) {
    if (!all(alpha < 1) || !all(alpha > 0))
      stop("All elements of alpha must be in (0,1)")

    # NB. these lines check that nboot > (1 / alpha) and because otherwise
    # you need more samples to get a somewhat useful confidence interval.
    alpha_sorted <- sort(alpha)
    if (nboot <= 1/min(alpha_sorted[1],1-alpha_sorted[length(alpha_sorted)]))
      warning("nboot is not large enough to estimate your chosen alpha.")

    # unrelated to the actual bootstrapping
    call <- match.call()

    # compute theta(x) of the samples and resample the data nboot times
    n <- length(x)
    thetahat <- theta(x,...)
    bootsam<- matrix(sample(x,size=n*nboot,replace=TRUE),nrow=nboot)

    # compute theta for each sample and compute the quartile of the fraction
    # below our original estimate under a normal distribution
    thetastar <- apply(bootsam,1,theta,...)
    z0 <- qnorm(sum(thetastar<thetahat)/nboot)

    # get a jackknife estimate for theta to compute the acceleration factor
    u <- rep(0,n)
    for(i in 1:n){
        u[i] <- theta(x[-i],...)
    }
    uu <- mean(u)-u
    acc <- sum(uu*uu*uu)/(6*(sum(uu*uu))^1.5)

    # compute the actual distribution that we are taking the quantiles of to
    # create the confidence interval
    zalpha <- qnorm(alpha)

    tt <- pnorm(z0+ (z0+zalpha)/(1-acc*(z0+zalpha)))

    confpoints <- quantile(x=thetastar,probs=tt,type=1)

    # and now just some logic for outputting it
    names(confpoints) <- NULL
    confpoints <- cbind(alpha,confpoints)
    dimnames(confpoints)[[2]] <- c("alpha","bca point")
    return(list(confpoints=confpoints,
                z0=z0,
                acc=acc,
                u=u,
                call=call))
}
```

```{r , echo=FALSE, cache = FALSE}

#------------ BCa interval for skewness using bcanon
bca_skew <- bcanon(
  x     = z_n_clean,
  nboot = N_reps,
  alpha = c(.025,.975),
  theta = e1071::skewness,
  type  = 1
)

# calculating quantile based confidence interval
print(quantile(skew_bootstrap, probs = c(0.025, 0.975)))
print(quantile(skew_normal_sim, probs = c(0.025, 0.975)))

# alpha and corresponding BCa points
print(bca_skew$confpoints)

skew_boot_full <- replicate(
  N_reps,
  e1071::skewness(sample(z_n_clean, length(z_n_clean), replace = TRUE), type = 1)
)

quantile(skew_boot_full, c(0.025, 0.975))

#------------ 95% CIs for comparison ----------

# (1) Normal-based parametric CI via simulation, n = 13
el2_ci_normal_95 <- quantile(skew_normal_sim, probs = c(0.025, 0.975))

# (2) Bootstrap percentile CI from empirical data, n = 13
el2_ci_boot_95 <- quantile(skew_bootstrap, probs = c(0.025, 0.975))

# (3) (bonus) BCa bootstrap CI for skewness using full data, n = 1651
bca_skew <- bcanon(
  x     = z_n_clean,
  nboot = N_reps,
  theta = e1071::skewness,
  type  = 1,
  alpha = c(0.025, 0.975)
)

el2_ci_bca_95 <- bca_skew$confpoints[, "bca point"]

ci_table <- data.frame(
  Method = c(
    "Simulated normal percentile",
    "Bootstrap percentile",
    "BCa bootstrap (full data)"
  ),
  N = c(
    n_sample_size,
    n_sample_size,
    length(z_n_clean)
  ),
  Lower = c(
    el2_ci_normal_95[1],
    el2_ci_boot_95[1],
    el2_ci_bca_95[1]
  ),
  Upper = c(
    el2_ci_normal_95[2],
    el2_ci_boot_95[2],
    el2_ci_bca_95[2]
  )
)
```

```{r skewness-95CI, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  ci_table,
  caption = "Comparison of 95 percent confidence intervals for skewness under different approaches.",
  booktabs = TRUE,
  align = c("l", rep("r", 3)),
  digits = 3,
  row.names = FALSE,
  escape = FALSE
) |>
  add_header_above(
    c(" " = 2, "95 percent CI" = 2),
    escape = FALSE
  ) |>
  kable_styling(full_width = FALSE, position = "center")
```

### Visual comparison and interpretation

An overlapping density plot was used to illustrate the differences between the
two skewness simulations.

```{r skewness-densities, fig.cap = paste0("Sampling distributions of the skewness estimator for ",N_reps, " samples of size n=", n_sample_size, " taken from a Normal model (blue) and from the empirical log-returns $z_n$ via bootstrap (red).")}

# bootstrapping skewness simulation
skew_df <- data.frame(
  skewness = c(skew_normal_sim, skew_bootstrap),
  Source   = factor(
    rep(c("Normal model", "Bootstrap from data"),
        each = N_reps)
  )
)

ggplot(skew_df, aes(x = skewness, fill = Source)) +
  geom_density(alpha = 0.4) +
  theme_bw() +
  labs(
    title = expression("Sampling distributions of " * hat(gamma)[1] *
                       " for n = 13"),
    x     = expression(hat(gamma)[1]),
    y     = "Density"
  )
```

As shown in Figure \@ref(fig:skewness-densities), the "Simulated Normal"
distribution is roughly centred on 0. The "Bootstrap Data" distribution is
slightly shifted to the left (lower than 0) (indicating the data's inherent
skew) and is it wider, with fatter tails.

!BE note: Here we have done two separate things.

- The first is the task specified in the document, bootstrapping the data using
  a subsampling approach (sometimes called m-out-of-n)
- The second is bootstrapping the data using the full data

- I think our data are iid (independent and identically distributed - confirmed
  in lab 2), or we can assume as much. We are estimating skewness -> I think the
  n-out-of-n (or full) bootstrap can be used.
  - Generally for CI

The implementation for this report utilised caching to store the R objects
generated in a code chunk to a cache database.

Refs to use, for jackknife bootstrap

- https://sites.stat.washington.edu/courses/stat527/s14/readings/ann_stat1979.pdf
- (general bootstrap intervals)
  https://projecteuclid.org/journals/statistical-science/volume-11/issue-3/Bootstrap-confidence-intervals/10.1214/ss/1032280214.pdf

The sampling distribution of skewness under the simulated Normal condition is
approximately centred at 0, as expected. The bootstrap distribution based on the
observed $z_{n}$ data is shifted to the left (mid-point \<0), reflecting a
negative skew, and is noticeably more dispersed with heavier tails.

The small samples drawn from the empirical $z_n$ distribution tend to produce
more extreme and variable skewness values than one would expect under a normal
assumption with the same mean and variance, even accounting for the small sample
size. This observation is in agreement with the Anderson-Darling test
(\S\@ref(elementOneAdtest)) and provides further evidence against an assumption
of normality for these data.

## Investigation of constant mean {#elementThr}

### Instructions (to be removed later)

_Divide your_ $z$ data into 14 subsamples, each representing six months. (If
your data cleansing has removed all or most of one six-month period, just omit
it from this analysis.) Compare the means of your subsamples using an
appropriate illustration, supported by at least one statistical test. [Note that
Analysis of Variance (ANOVA) is based on an assumption of Normality: if you
decide to use ANOVA for your test you will need to comment on the Normality, or
otherwise, of your 7 subsamples.]

### Visual comparison of means

Want to test second core assumption that the log-returns $z_n$ have a constant
mean $\mu$\dots

Want to use split() function to divide the data by time periods to create a list
of 14 xts objects, each representing a 6-month period. The number of data points
per six-month period is shown in Table \@ref(tab:days-per-six-month-period),
located in the Appendix.

```{r Element 3 pt1 - TBC}
#----------------- Element 3: investigation of constant mean ------------------#
# # ---------------- attempt 01: using flexible 6 month windows
# # split z data into 14 subsamples
# z_list <- split(z_n_clean, f = "months", k = 6)
#
# # drop values if all or most of 1 6M period have been removed
# # BE note: have just removed fully blank values, but need to tweak this
# z_list <- z_list[lengths(z_list) > 0]

# ---------------- attempt 02: new (using hard 6 month cut-offs)

# ensure we have dates
dates <- index(z_n_clean)

# start - first 6-month block that contains the first date
first_date <- min(dates)
fy <- as.integer(format(first_date, "%Y"))
fm <- as.integer(format(first_date, "%m"))
start0 <- if (fm <= 6) {
  as.Date(sprintf("%d-01-01", fy))
} else {
  as.Date(sprintf("%d-07-01", fy))
}

# end - first boundary after the last date
last_date <- max(dates)
ly <- as.integer(format(last_date, "%Y"))
lm <- as.integer(format(last_date, "%m"))
end_next <- if (lm <= 6) {
  as.Date(sprintf("%d-07-01", ly))
} else {
  as.Date(sprintf("%d-01-01", ly + 1))
}

# fixed 6-month breakpoints
breaks <- seq(start0, end_next, by = "6 months")

# nice labels "01 Jan 2018 - 30 Jun 2018" etc.
start_labels <- breaks[-length(breaks)]
end_labels   <- breaks[-1] - 1

period_labels <- paste(
  format(start_labels, "%b %Y"),
  "-",
  format(end_labels,   "%b %Y")
)

# assign each observation to a 6-month bin (right-open [start, end])
Period <- cut(
  dates,
  breaks = breaks,
  right  = FALSE,
  labels = period_labels,
  include.lowest = TRUE
)

# create list of subsamples
z_list <- split(z_n_clean, Period)

# drop empty periods (where cleaning removed all data)
z_list <- z_list[lengths(z_list) > 0]

combined_df_el3 <- bind_rows(lapply(seq_along(z_list), function(i) {
  data.frame(
    Period  = factor(names(z_list)[i], levels = names(z_list)),
    z_value = as.numeric(z_list[[i]])
  )
}))
```

```{r element3-boxplot, echo=FALSE, fig.cap = "TBC: boxplots showing log-returns of IXJKFLDS! over six-month periods."}
ggplot(combined_df_el3, aes(x = Period, y = z_value)) +
  geom_boxplot() +
  theme_bw() +
  labs(
    title = "Log-returns over six-month periods",
    x = "Six-month period",
    y = expression(z[n])
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Statistical test

```{r}
kruskal_res <- kruskal.test(z_value ~ Period, data = combined_df_el3)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kruskal_df <- data.frame(
  Test = "Kruskal–Wallis rank sum test",
  Df = kruskal_res$parameter,
  Statistic = unname(kruskal_res$statistic),
  p.value = kruskal_res$p.value
)

kruskal_df$Statistic <- sprintf("%.3f", kruskal_df$Statistic)
kruskal_df$p.value <- formatC(kruskal_df$p.value, format = "fg", digits = 4)
```

```{r kruskal-res, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  kruskal_df,
  caption = "Kruskal–Wallis test outcome for differences in log-return distributions across six-month periods.",
  booktabs = TRUE,
  align = c("l", "r", "r", "r"),
  row.names = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")
```

From \S\@ref(elementOne) & \@ref(elementTwo) we know that our data cannot be
treated as being normally distributed/shows strong departures from Normality. We
therefore opted to use a non-parametric alternative to one-way ANOVA.

<!-- We therefore cannot use ANOVA testing as it is based upon an assumption of normality. -->

The Kruskal-Wallis test was used as a non-parametric alternative to one-way
ANOVA to compare the behaviour of $z_n$ across the six-month periods.

The Kruskal–Wallis test compares the distributions (equivalently, the mean
ranks) of the groups. Under the null hypothesis $H_0$, all groups share the same
distribution. If we assume similar spread and shape, this can be interpreted as
equality of medians (ref!).

    - Comment on Kruskal-Wallis assumptions (independence element can be referenced)

This checks for differences in central tendency across groups and the null
hypothesis ($H_0$) is that the medians of all 14 groups are equal.

As shown in Table \@ref(tab:kruskal-res), there are no statistically significant
differences in the typical (central) log-return across the periods
(p=`r signif(kruskal_res$p.value, 3)`). We therefore accept the null hypothesis,
$H_0$, that (!TBI). This finding is consistent with the assumption of a constant
mean (or median) log-return over time.

```{=latex}
\newpage
```

## Investigation of constant variance {#elementFou}

<!-- ### Identifying subsamples with max/min variance (chapter 4 in CS1) -->

```{r Element 4 pt1 - processing, echo=FALSE}
#--------------- Element 4: investigation of constant variance ----------------#
variances <- sapply(z_list, var, na.rm = TRUE)
min_var_loc <- which.min(variances)
max_var_loc <- which.max(variances)

# set up variance dataframe
var_df <- data.frame(
  Period   = factor(names(z_list), levels = names(z_list)),
  Variance = as.numeric(variances)
)

# !changed to *252: extract the subsamples as numeric vectors & drop NA
min_var_sample <- na.omit(as.numeric(z_list[[min_var_loc]]))*252
max_var_sample <- na.omit(as.numeric(z_list[[max_var_loc]]))*252
min_var_sample <- na.omit(as.numeric(z_list[[min_var_loc]]))
max_var_sample <- na.omit(as.numeric(z_list[[max_var_loc]]))

# create bootstrap statistic function to get sample variance of resampled data
boot_var <- function(data, indices) {
  var(data[indices])
}

# run separate bootstrapping for both subsamples
boot_var_min <- boot(
  data      = min_var_sample,
  statistic = boot_var,
  R         = 5000
)

boot_var_max <- boot(
  data      = max_var_sample,
  statistic = boot_var,
  R         = 5000
)

# get CI using Bias-Correct and Accelerated (bca) interval
# (this adjusts for both bias and skewness in bootstrap distribution)
var_ci_min_95 <- boot.ci(boot_var_min, type = "bca", conf = 0.95)
var_ci_max_95 <- boot.ci(boot_var_max, type = "bca", conf = 0.95)
# 99% CI for interest
var_ci_min_99 <- boot.ci(boot_var_min, type = "bca", conf = 0.99)
var_ci_max_99 <- boot.ci(boot_var_max, type = "bca", conf = 0.99)
```

```{r Element 4 pt2 - table, echo=FALSE}
# Period labels for min/max variance subsamples
min_period <- names(z_list)[min_var_loc]
max_period <- names(z_list)[max_var_loc]

# Extract BCa 95% CI limits from boot.ci output
# (bca columns: conf, lower, upper, etc. We want the low & high limits.)
var_ci_min_bca <- var_ci_min_95$bca[4:5]
var_ci_max_bca <- var_ci_max_95$bca[4:5]

# getting end date
# start date of each block
start_dates <- as.Date(sapply(z_list, function(x) index(x)[1]))

# last observed date in each block
last_dates  <- as.Date(sapply(z_list, function(x) index(x)[NROW(x)]))

# end date of each block:
# - for all but last: day before next block's start
# - for last: its own last date
end_dates <- c(start_dates[-1] - 1, last_dates[length(last_dates)])
period_labels <- paste(
  base::format(start_dates, "%b %Y"),
  "-",
  base::format(end_dates, "%b %Y")
)

var_df <- data.frame(
  Period   = factor(period_labels, levels = period_labels),
  Variance = as.numeric(variances)
)

min_period_label <- period_labels[min_var_loc]
max_period_label <- period_labels[max_var_loc]

```

```{r element4-violin-bar, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions and variability of cleaned log-returns across six-month periods. Top: violin and boxplot summaries of $z_n$ for each period. Bottom: corresponding sample variances $s^2$, highlighting relative differences in spread."}
# replotting box plot from investigation of constant mean above new variance bar
# chart

# boxplot
p_box_el4 <- ggplot(combined_df_el3, aes(x = Period, y = z_value)) +
  geom_boxplot(width = 0.75, outlier.size = 0.7, linewidth   = 0.3) +
  theme_bw() +
  labs(
    title = "Log-returns over six-month periods",
    x = "Six-month period",
    y = expression(z[n])
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_blank(),
    axis.ticks.x    = element_blank(),
  )

p_var_el4 <- ggplot(var_df, aes(x = Period, y = Variance, fill = Period)) +
  # scale_fill_viridis_d() +
  geom_col(fill = "white", colour = "black", width = 0.75, linewidth   = 0.3) +
  theme_bw() +
  labs(
    title = "Sample variance of log-returns by six-month period",
    x = "Six-month period",
    y = expression(s^2)
  ) +
  # scale_y_continuous(limits = c(0, 5e-04), expand = c(0,0)) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_text(angle = 45, hjust = 1),
    legend.position = "none",
  )

# Stack vertically with aligned x-axis
library(patchwork)
(p_box_el4 / p_var_el4) +
  plot_layout(heights = c(1, 1))
```

```{r element4-box-bar, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions and variability of cleaned log-returns across six-month periods. Top: boxplot summaries of $z_n$ for each period. Bottom: corresponding sample variances $s^2$, highlighting relative differences in spread."}
# # Violin + boxplot for spread/shape
# p_violin <- ggplot(combined_df_el3, aes(x = Period, y = z_value)) +
#   geom_violin(trim = FALSE, fill = NA, colour = "grey40") +
#   geom_boxplot(width = 0.08, outlier.size = 0.5) +
#   theme_bw() +
#   labs(
#     title = "Distribution of log-returns across six-month periods",
#     x = NULL,
#     y = expression(z[n])
#   ) +
#   theme(
#     axis.text.x = element_blank(),  # hide x labels here
#     axis.ticks.x = element_blank()
#   )

# # Bar chart of sample variances
# p_var <- ggplot(var_df, aes(x = Period, y = Variance)) +
#   geom_col() +
#   theme_bw() +
#   labs(
#     title = "Sample variance of log-returns by six-month period",
#     x = "Six-month period",
#     y = expression(s^2)
#   ) +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
#

p_violin <- ggplot(combined_df_el3, aes(x = Period, y = z_value, fill = Period)) +
  geom_violin(trim = TRUE, colour = "grey20", alpha = 0.4) +
  geom_boxplot(
    width = 0.2,
    outlier.size = 0.5,
    fill = "white",
    colour = "black"
  ) +
  scale_fill_viridis_d() +
  theme_bw() +
  labs(
    title = "Distribution of log-returns across six-month periods",
    x = NULL,
    y = expression(z[n])
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_blank(),
    axis.ticks.x    = element_blank(),
    legend.position = "none"
  )

p_var <- ggplot(var_df, aes(x = Period, y = Variance, fill = Period)) +
  geom_col(alpha = 0.95) +
  scale_fill_viridis_d() +
  theme_bw() +
  labs(
    title = "Sample variance of log-returns by six-month period",
    x = "Six-month period",
    y = expression(s^2)
  ) +
  theme(
    # panel.grid = element_blank(),
    # panel.grid.major.y = element_blank(),
    # panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x     = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )

# Stack vertically with aligned x-axis
(p_violin / p_var) +
  plot_layout(heights = c(1, 1))
```

<!-- ### Bootstrapping confidence intervals for $\sigma^2$ -->

```{r var-ci, echo=FALSE, message=FALSE, warning=FALSE}
# Create summary table
var_ci_table <- data.frame(
  Subsample = c("Min variance", "Max variance"),
  Period = c(min_period_label, max_period_label),
  s2        = c(var(min_var_sample),
                var(max_var_sample)),
  CI_lower  = c(var_ci_min_bca[1],
                var_ci_max_bca[1]),
  CI_upper  = c(var_ci_min_bca[2],
                var_ci_max_bca[2])
)

fmt <- function(x) formatC(x, format = "f", digits = 6)

var_ci_table_disp <- var_ci_table %>%
  mutate(
    s2       = paste0("$", fmt(s2), "$"),
    CI_lower = paste0("$", fmt(CI_lower), "$"),
    CI_upper = paste0("$", fmt(CI_upper), "$")
  )

kable(
  var_ci_table,
  col.names = c(
    "Subsample",
    "Six-month period",
    "Sample variance",
    "95\\% CI lower",
    "95\\% CI upper"
  ),
  booktabs = TRUE,
  align = c("l", "l", "r", "r", "r"),
  escape = FALSE,
  digits = 6,
  caption = "Bootstrap bias-corrected and accelerated 95\\% confidence intervals for the sample variance $s^2$ in the six-month periods with minimum and maximum sample variance."
) %>%
  kable_styling(full_width = FALSE, position = "center")

# kable(
#   var_ci_table,
#   caption = "Bootstrap BCa 95\\% confidence intervals for the variance $s^2$ in the six-month periods with the smallest and largest sample variance of log-returns.",
#   booktabs = TRUE,
#   align = c("l", "r", "r", "r"),
#   escape = FALSE
# ) |>
#   kable_styling(full_width = FALSE, position = "center")
```

<!-- ### Commenting on overlap -->

### Instructions (to be removed later)

_Use the same subsamples as for Element 3. Use a suitable illustration to
compare the spread of your subsamples. Identify the six-month period with the
largest sample variance and the six-month period with the smallest sample
variance. Use bootstrapping/resampling to construct a 95% confidence interval
for_ $\sigma^2$ for each of these two subsamples separately. Comment on any
perceived overlap between these intervals.

### Analysis... (!change)

The six-month period with the largest sample variance is
`r names(z_list)[min_var_loc]` and the six-month period with the smallest sample
variance is `r names(z_list)[max_var_loc]`.

!Paragraph here describing the method that we used to address each point we are
asked about

The 95% bias-corrected and accelerated (BCa) confidence intervals (CI) for the
variance ($\sigma^2$) for each of these two subsamples are shown in Table
\@ref(tab:var-ci). There is no overlap between the 95% CI for the 6-month ranges
with the minimum and maximum variance
[`r formatC(var_ci_min_bca[1], format = "fg", digits = 4)`,
`r formatC(var_ci_min_bca[2], format = "fg", digits = 4)`] and
[`r formatC(var_ci_max_bca[1], format = "fg", digits = 4)`,
`r formatC(var_ci_max_bca[2], format = "fg", digits = 4)`], respectively. Or in
scientific notation the 95% CI for the minimum variance subsample is
[`r formatC(var_ci_min_bca[1], format = "e", digits = 2)`,
`r formatC(var_ci_min_bca[2], format = "e", digits = 2)`] and the 95% CI for the
maximum variance subsample is
[`r formatC(var_ci_max_bca[1], format = "E", digits = 2)`,
`r formatC(var_ci_max_bca[2], format = "E", digits = 2)`].

This finding provides statistical evidence that the true variance $\sigma^2$ is
not the same in both periods. We have evidence therefore to reject the
assumption of constant variance in these data.

!BE note: check to see if can make this statement about CI's NOT overlapping.
Know that have to be careful about the statements made here.

<!-- $\big[`r formatC(var_ci_min_bca[1], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`, -->

<!-- `r formatC(var_ci_min_bca[2], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`\big]$ -->

<!-- and -->

<!-- $\big[`r formatC(var_ci_max_bca[1], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`, -->

<!-- `r formatC(var_ci_max_bca[2], format = "fg", digits = 4,small.mark = " ", small.interval = 5L)`\big]$, -->

<!-- respectively. -->

## Independence of increments {#elementFiv}

### Instructions (to be removed later)

_The Efficient Markets Hypothesis states that the price of an asset today
incorporates all the information which is available about the asset, and
therefore that any changes to the price between today and tomorrow will be based
on new, and unpredictable, information which arrives in the meantime. The
practical result of this is that_ $z_{n}$ _should be independent of both_
$z_{n−1}$ and $y_{n−1}$ _if the EMH is true._

_Test the independence of_ ${z_{n}}$ _and_ ${z_{n-1}}$ _by using a contingency
table procedure. Let_ $q_1$, $q_2$, $q_3$ _represent the sample quartiles of
the_ $z$ _data. Classify each of the_ $z_n$ _observations according to whether
it is less than_ $q_1$_, between_ $q_1$ _and_ $q_2$_, between_ $q_2$ _and_
$q_3$_, or greater than_ $q_3$_. Now draw up the contingency table to determine
whether the classification of_ $z_{n}$ _is dependent on the classiﬁcation of_
$z_{n−1}$.

### Data preparation

Want to test fourth assumption - the $z_n$ increments are independent of each
other.

Test for dependence between $z_n$ and its immediate predecessor, $z_{n-1}$.

Test of the Efficient Markets Hypothesis, which implies that past returns
($z_{n-1}$) should not predict future returns ($z_n$)

<!-- # ```{r format-p-values} -->

<!-- # # BE note: can move this up, but have put here to discuss -->

<!-- # # want to ask Russell -->

<!-- # format.pval(one$p.value, eps = 0.0001, scientific = FALSE) -->

<!-- # ``` -->

```{r Element 5 pt1 - independence of increments}
#------------------- Element 5: independence of increments --------------------#
# Create a $z_{n-1}$ series
z_lag <- stats::lag(z_n_clean, k = 1)

# Align $z_n$ and $z_{n-1}$ pairs by merging the xts objects
# inner join to remove the NA value created by the lag
z_merged <- merge(z_n_clean, z_lag, join = "inner")
colnames(z_merged) <- c("z_n", "z_n_minus_1")

# Categorise by quartile
# calculate quartiles from whole z_n_clean dataset
q <- quantile(z_n_clean, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

# create categorical variables
breaks  <- c(-Inf, q, Inf)
labels  <- c("Q1", "Q2", "Q3", "Q4")
z_n_cat <- cut(z_merged$z_n, breaks = breaks, labels = labels)
z_n_minus_1_cat <- cut(z_merged$z_n_minus_1, breaks = breaks, labels = labels)

# contingency
# rows $z_{n-1}$, columns $z_n$
con_table <- table(z_n_minus_1_cat, z_n_cat)

# statistical test (want Chi-squared, but need to see if cell frequencies are >5)
chisq_result <- chisq.test(con_table)
chisq_result
chisq_result$expected
# If any expected counts are low (R will often produce a warning), the $\chi^2$ p-value is unreliable.
# may then want to use Fisher's Exact Test
if (any(chisq_result$expected < 5)) {
  fisher_result <- fisher.test(con_table)
  fisher_result
}
#
```

```{r contingency-table, echo=FALSE, message=FALSE, warning=FALSE}
con_table_df <- as.data.frame.matrix(con_table)

con_table_df <- tibble::tibble(
  `$z_{n-1}$ quartile` = rownames(con_table),
  con_table_df
)

kable(
  con_table_df,
  caption = "Contingency table of quartile classifications for lagged log-returns $z_{n-1}$ (rows) and current log-returns $z_n$ (columns).",
  booktabs = TRUE,
  align = c("l", rep("r", 4)),
  escape = FALSE
) |>
  add_header_above(
    c(" " = 1, "$z_n$ quartile" = 4),
    escape = FALSE
  ) |>
  kable_styling(full_width = FALSE, position = "center")
```

None of the cell frequencies are \<5 so we can use chi-squared test.

All expected cell frequencies exceed 5, so the assumptions for Pearson’s
chi-squared test of independence are satisfied.

The null hypothesis ($H_0$) for both tests is that the row and column variables
are independent - here that the categorical distribution of the current
log-return $z_n$ is independent of hte distribution of the lagged log-return
$z_{n-1}$.

The test returns $\chi^2$(\text{df} = `r chisq_result$parameter`) =
`r formatC(unname(chisq_result$statistic), format = "f", digits = 2)` with !p
`r format.pval(chisq_result$p.value, eps = 0.0001, scientific = FALSE)`
(p=`r formatC(chisq_result$p.value, format = 'e', digits = 3)`), so we reject
$H_0$. This provides statistical evidence that $z_n$ is dependent on the
classification of $z_{n-1}$ and this finding does not not support the Efficient
Markets Hypothesis.

## General upwardness of trend {#elementSix}

### Instructions (to be removed later)

_One aspect of the data which might reflect a general tendency for the index to
increase might be the proportion of days when the price increment is positive.
Test whether this proportion is greater than 0.5._

_Another aspect is the persistence of trends. A “positive run” is a sequence of
consecutive days when_ $Z_i > 0$; a “negative run” is a sequence of consecutive
days when $Z_i < 0$. (Note that a run may only be one day in length, or may be
longer than one day.) Identify the runs in your data set and test whether
positive runs have longer mean duration than negative runs.

### Proportion of positive increments

Want to test whether proportion is \>0.5 (i.e., occurs over 50% of the time).

```{r Element 6 pt1 - proportion of positive increments}
#------------------- Element 6: general upwardness of trend -------------------#
num_positive <- sum(z_n_clean > 0)
total_days <- length(z_n_clean)
# test using binomial (could also approximate, but Russell normally says
# 'why approximate when you can calculate' so opted for that here)
binom.test(num_positive, total_days, p = 0.5, alternative = "greater")
```

(Assuming this code is valid, but this seems too easy so please check)

Null hypothesis is that the proportion is 50%.

!P-value is below 0.001 (insert values from R) so can reject $H_0$ in favour of
alternative hypothesis, which is that the true probability of success is greater
than 0.5.

The proportion of positive-return days is statistically greater than 50%,
confirming a general upward bias.

### Persistence of trends

Test hypothesis that positive runs (consecutive $z_n > 0$) have a longer mean
duration than negative runs (consecutive $z_n < 0$)

```{r Element 6 pt2 - persistence of trends}
#----------------------------- Element 6: part 2 ------------------------------#
# use run length encoding
# changing to using z_n_clean_numeric to avoid error
# 'rle(signs) : 'x' must be a vector of an atomic type'
signs <- sign(z_n_clean_numeric)
runs <- rle(signs)

# runs object contains $values (1 for positive, -1 for negative) and
# $lengths (the duration of each run)
pos_run_lengths <- runs$lengths[runs$values == 1]
neg_run_lengths <- runs$lengths[runs$values == -1]

mean(pos_run_lengths)
mean(neg_run_lengths)

# could use t-test, but unlikely these are normally distributed
# non-parametric alternative is Mann-Whitney test (/Wilcoxon Rank-Sum)
# -> want to compare distributions of two groups without assuming normality
# Onesided here I think

wilcox.test(pos_run_lengths, neg_run_lengths, alternative = "greater")
```

Null hypothesis is that the mean of positive and negative runs are equal.
Alternative hypothesis is that the mean of positive runs is greater (so one
tailed).

Wilcoxon rank sum test with continuity correct has !p-value \< 0.001 so we have
sufficient evidence to reject the null hypothesis in favour of the alternative
hypothesis (the positive runs have a shift greater than 0).

!BE note: want to include the values from R in the text again.

# Conclusions

While the log-normal model was invalidated by the data's non-normality
(\S\@ref(elementOne) & \S\@ref(elementTwo)), non-constant variance
(\S\@ref(elementFou)), and lack of independence (\S\@ref(elementFiv)), our
analysis for \S\@ref(elementThr) did not find statistical evidence to reject the
assumption of a constant mean.

To be updated:

Based on the Kruskal-Wallis Test the mean log-return appears to be constant over
time, even as the other core assumptions are violated.

!BE note/summary:

- element 1 (\S\@ref(elementOne)): Anderson-Darling test provides strong
  evidence that the $z_n$ data are not normally distributed
- element 2 (\S\@ref(elementTwo)): bootstrapping with z-data and normal
  distribution also provides evidence against an assumption of normality
- element 3 (\S\@ref(elementThr)): Kruskal–Wallis test $\therefore$ we accept
  null hypothesis that measures of central tendancy of log-returns over
  six-month periods share the same distribution (/medians are the same?)
- element 4 (\S\@ref(elementFou)): 95% BCa confidence intervals do not overlap,
  providing statistical evidence that (true) variance is not the same in the
  minimum and maximum six-month periods of sample variable evaluated
- element 5 (\S\@ref(elementFiv)): Chi-squared test provided evidence that $z_n$
  and $z_{n-1}$ are not indendent. BUT haven't checked for $y_{n-1}$ as not sure
  what this is...
- element 6 (\S\@ref(elementSix)): Part 1: Binomial test revealed proportion of
  positive-return days is statistically greater than 50% (insert p \<0.001),
  confirming a general upward bias. Part 2: Wilcoxon rank sum test with
  continuity correction provided sufficient statistical evidence (p\<0.001) to
  reject null hypothesis in favour of alternative hypothesis that the positive
  runs have a greater mean/median/distribution that the negative runs.

Want to wrap these up into some sort of formal conclusion...

- Something to discuss

```{=latex}
\newpage
```

# Appendix

## Accessibility

An accessible HTML version of this report is available via a public GitHub page:

- https://ytterbiu.github.io/a_group07-project

## Methodology for COVID-19 exclusion identification {#apd-COVID19-selection}

Rough notes:

- LU 1 – shows existing pre-COVID up trend
- LD L1 shows - panic selling induced down trend.
- 24rd Feb 2020 uptrend shown by line LU1 was breached as shown by point A.
- This was right after Italy declaring lockdown.
- 25th Feb 2020 – US CDC Telebriefing indicating mitigating measures.
- 13 March 2020 – Trump Administration declares nationwide emergency.
- 23rd March 2020 down trend shown by line LD1 was breached as shown by point B.
- We can assume the market has absorbed the shock of announcement of COVID-19.
- Line LU2 Captures on upcoming Uptrend

```{r, echo=FALSE, out.width="70%", fig.cap="Changes in the Nasdaq Composite index from x – y.  Analysis was carried out to..."}
knitr::include_graphics("fig/appendix-COVID19-selection.pdf")
```

## Additional tables and figures

In \S\@ref(elementThr) the log-returns data were split into six-month periods,
as shown in Figure \@ref(fig:element3-boxplot).

```{r, echo=FALSE, out.width="70%"}
knitr::include_graphics("fig/element3-boxplot-1.pdf")
```

```{r appendix-setup, echo=FALSE, message=FALSE, warning=FALSE}
#---------- Appendix: check & count observations in 6-month periods -----------#
# Count observations in each 6-month period
days_per_period <- sapply(z_list, NROW)

period_labels <- names(days_per_period)

# split into start/end strings
start_dates <- sub(" -.*", "", period_labels)
end_dates   <- sub(".*- ", "", period_labels)

days_table <- data.frame(
  `Start date`             = start_dates,
  `End date`               = end_dates,
  `Number of data entries` = as.integer(days_per_period),
  check.names = FALSE
)
```

```{r days-per-six-month-period, echo=FALSE, message=FALSE, warning=FALSE}
kable(
  days_table,
  col.names = c(
    "Start date",
    "End date",
    "Number of data points post cleaning"
  ),
  caption = "Number of non-missing log-return observations in each six-month period.",
  booktabs = TRUE,
  longtable = TRUE,
  align = c("r","r", "c")
) |>
  kable_styling(
    full_width = FALSE,
    position = "center"
  )
```

<!-- BE note: taken out as we are submitting R file separately -->
<!-- Use: `knitr::purl("cs1-group07.rmd")` to generate r file -->
<!-- ## R Code Documentation -->
<!-- ```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

``` -->

## Generative AI Declaration

Generative AI tools were used to assist with project planning and methodological
validation throughout this project.

### Tools Used

Google Gemini 2.5 Pro (Deep Research), ChatGPT 5

### Prompts used

The GenAI tools were provided with the 'Coursework Group 7.pdf' and extracts
from our R code.

A non-exhaustive list of prompts/queries used is shown below. We opted to
include at least one example of each _type_ of query used, rather than including
all queries used. For example, if we entered the prompts/queries 'How can we put
two ggplots side by side?' and 'How to remove grey background on a ggplot', we
would only include the first of these in the list below, as both relate to
plotting issues (& are similar to querying with a traditional search engine).

- "Can you help with outlining the steps for this project."
- "...the top of both graphs in r are slightly different. (Pasting in R code.)
  Can this be fixed for the plot area?"
- "Help phrasing this in a simple and accessible way - remove any jargon &
  identify the key points"
- "How can this error be tracked down in an rmd file? ! Text line contains an
  invalid character. l.1 ^^@^^@..."
- ...

### Use of GenAI outputs & subsequent revisions

The GenAI tools served as a additional group member. Notably, the GenAI output:

- Helped debug plotting issues.
- Raised potential statistical traps (e.g., the "ANOVA trap" and the "data
  cleansing order" trap).
- Suggested appropriate non-parametric tests (Kruskal-Wallis, Wilcoxon Rank-Sum)
  where the assumptions of parametric tests (ANOVA, t-test) were violated.
- Recommended specific R packages (nortest, e1071) and functions (ad.test,
  kruskal.test) best suited for each task.
- Commented on the structure of R code snippets and the report structure.

### Changes made:

The AI's suggestions were critically reviewed and debated by the group. Plotting
formatting in ggplot, ... (add others) and... were all implemented following an
iterative feedback cycle.

The R code for implementation was written and debugged by the group. GenAI tools
were used, and thought of, in the capacity of an additional group member. All
final analysis, interpretation, and writing in this report were generated by
(human) group members.

---

[^log-symmetry]:
    With simple percentages a 100% gain (+100%) is cancelled out by a 50% loss
    (−50%), not a 100% loss (−100%). This asymmetry makes statistical modeling
    difficult. With log-returns, a move from $100 to $200 (a log-return of
    +0.693) is the exact mathematical opposite of a move from $200 to $100 (a
    log-return of −0.693).

[^cont2001quote]:
    Where a 'stylized empirical fact' was defined as being "properties common
    across a wide range of instruments, markets and time periods" [@Cont01022001
    pp. 224]
