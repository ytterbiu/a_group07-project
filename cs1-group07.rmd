---
title: "SMM047 Group Coursework 2025-26"
subtitle: "Group 07"
author:
  - "Abdulrahman Alolyan"
  - "Benjamin Evans"
  - "Amogh Sharma"
  - "Nivetha Subbiah"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    self_contained: true
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
    df_print: paged
  # runtime: shiny
  pdf_document:
    includes:
      in_header: preamble.tex
    number_sections: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: true
# fontsize: 10pt
bibliography: references.bib
---

```{=latex}
\newpage
```

# Introduction

## Executive summary

Summary / Abstract here

<!-- ## Key Information

### Background

This R Markdown document was created as part of a group assignment for SMM047 at Bayes Business School, City St George's, University of London in Term 1 2025-26.

-   *BE note: use `rmarkdown::render("MSc_AS-SMM634-Group4-Project.rmd", output_dir = "docs")` to render this document* -->

## Initial processing

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.align = "center", fig.width = 7, fig.height = 7,
  out.width = "100%"
)
```

```{r, include=FALSE, echo=FALSE}
dir.create("fig", showWarnings = FALSE)
knitr::opts_chunk$set(
  fig.path   = "fig/",
  dpi        = 300,
  fig.width  = 6,
  fig.height = 4,
  dev        = "pdf"
)
if (knitr::is_latex_output()) {
  # For PDF output
  knitr::opts_chunk$set(
    fig.path   = "fig/",
    dpi        = 300,
    fig.width  = 6,
    fig.height = 4,
    dev        = "pdf"
  )
} else {
  # For HTML (or other)
  knitr::opts_chunk$set(
    fig.path   = "fig/",
    dpi        = 300,
    fig.width  = 6,
    fig.height = 4,
    dev        = "svglite"
    # or "png" if you prefer
  )
}
```

```{r, include=FALSE, echo=FALSE}
# Clean environment
rm(list = ls()) # Remove all objects
graphics.off() # Close all graphical devices
cat("\014") # Clean console
```

```{r, include=FALSE, echo=FALSE}
## dependencies / external librarys
library(quantmod)
```

The Nasdaq Composite (`^IXIC`) data was downloaded using the `getSymbols`
command from the `quantmod` library.

```{r Download data, include=FALSE}
# Download the data
getSymbols("^IXIC", from = "2018-01-01", to = "2024-12-31")
```

First we can prepare our data, getting the adjusted close price, applying a log
transform, calculating the log-return increment, and removing NaN values.

```{r Prepare data}
# get adjusted close price
IXIC_ad <- Ad(IXIC)
# log transform
y_n <- log(IXIC_ad)
# log-return increment (z_n) - get difference in log transformed data
z_n <- diff(y_n)
# remove NA values
z_n <- na.omit(z_n)
```

Let's have a quick look at the data.

```{r Initial visualisation}
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)

# same length as IXIC_ad, first value NA
z_n_xts <- diff(log(IXIC_ad))

df <- tibble(
  date  = index(IXIC_ad),
  price = as.numeric(IXIC_ad),
  z_n   = as.numeric(z_n_xts)
) %>%
# drops the first row where z_n is NA
  drop_na(z_n)
p1 <- ggplot(df, aes(date, price)) +
  theme_bw() +
  geom_line() +
  labs(
    title = "NASDAQ Composite\n (Adjusted Close)",
    x = "Date", y = "Adjusted Close"
  )

p2 <- ggplot(df, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  labs(
    title = expression(Log~Return~Increment~(z[n])),
    x = "Date",
    y = expression(z[n])
  )

# gridExtra::grid.arrange(p1, p2, ncol = 2)
(p1 | p2) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

```

# Individual elements

## Data cleaning and standard test of normality

### Identifying and removing outliers

We chose to remove data from ... (need to include paragraph here outlining how
we chose the dates used: 2020-03-01/2020-05-31 - BE note: these are placeholder
at the moment just to get the code to work... Something to discuss and justify)

It is suggested that we remove data from the 2020/21 COVID-19 period
($log~x_{20May2020}$ and $log~x_{09Apr2020}$), when markets were heavily
disrupted. The extreme, policy-driven movements in this window can be argued to
not reflect the “normal” behaviour of the IXIC index.

As noted in the assessment instructions, if we delete dates from the dataset, we
cannot then treat the jump between the last date before the gap and the first
date after the gap as a valid increment ($z_n$). Calculating log-returns first
and only afterwards drop rows for the excluded period, our data would contain
incorrect 'links'.

Cleaning has to happen before we compute any returns.

```{r Element 1 pt1 - cleaning}
#---------- Element 1: data cleaning and standard test of normality -----------#

outlier_period <- "2020-03-01/2020-05-31"
# set all values from outlier period to NA
y_n_clean <- y_n
y_n_clean[outlier_period] <- NA
# calculate log-return increments
z_n_clean <- diff(y_n_clean)
z_n_clean_pre_na_drop <- z_n_clean
# remove NA increments
z_n_clean <- na.omit(z_n_clean)
```

Quick visualisation to compare new vs old:

```{r Plot raw vs clean, echo=FALSE}
df_raw <- tibble(
  date = index(z_n),
  z_n  = as.numeric(z_n)
)
df_clean <- tibble(
  date = index(z_n_clean_pre_na_drop),
  z_n  = as.numeric(z_n_clean_pre_na_drop)
)
ylim_all <- range(df_raw$z_n, df_clean$z_n, na.rm = TRUE)


p_raw <- ggplot(df_raw, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(pre exclusion)",
    x = "Date",
    y = expression(z[n])
  )

p_clean <- ggplot(df_clean, aes(date, z_n)) +
  geom_line() +
  theme_bw() +
  scale_y_continuous(limits = ylim_all) +
  labs(
    title = "Log Return Increments\n(post exclusion)",
    x = "Date",
    y = expression(z[n])
  )

(p_raw | p_clean) &
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5)
  )

```

### Sample statistics

<!-- Skewness and excess kurtosis are computed using e1071 with type = 1. -->

Skewness and excess kurtosis of the cleaned log-returns $z_n$ were computed
using the `e1071` package in R with type = 1 [@R-e1071]. This corresponds to the
classical moment-based definitions $m_3/m_2^{3/2}$ and $m_4/m_2{^2}$ described
by @joanes1998, and used in SMM047 2025-26 [@gerrard_seg2_notes_questions].
These measures are widely used in textbooks and provide a direct description of
the shape of the empirical distribution.

Alternative software implementations, such as the package `summarytools`, were
considered. These packages apply small-sample bias corrections (“type 3”
estimators), which lead to slightly different values but are asymptotically
equivalent. Given our relatively large sample size and descriptive aim, the type
1 definitions are appropriate and fully adequate.

```{r Element 1 pt2}
library(e1071)
library(kableExtra)
# e1071::skewness(z_n_clean, type = 1)

z_n_clean_numeric <- as.numeric(z_n_clean)

desc <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness (Type 1)",
    "Excess Kurtosis (Type 1)"
  ),
  Value = c(
    mean(z_n_clean_numeric),
    var(z_n_clean_numeric),
    e1071::skewness(z_n_clean_numeric, type = 1),
    # already excess kurtosis for type = 1
    e1071::kurtosis(z_n_clean_numeric, type = 1)
  )
) |>
  mutate(Value = sprintf("%.6f", Value))

kable(
  desc,
  caption = "Descriptive statistics of cleaned log returns ($z_n$).",
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")

```

BE note: I have included a quick demo of this here:

```{r Element 1 pt3, echo=FALSE}
library(e1071)
library(kableExtra)
# e1071::skewness(z_n_clean, type = 1)

z_n_clean_numeric <- as.numeric(z_n_clean)

desc <- tibble(
  Statistic = c(
    "Sample Mean",
    "Sample Variance",
    "Skewness (Type 1)",
    "Skewness (Type 2)",
    "Skewness (Type 3)",
    "Excess Kurtosis (Type 1)",
    "Excess Kurtosis (Type 2)",
    "Excess Kurtosis (Type 3)"
  ),
  Value = c(
    mean(z_n_clean_numeric),
    var(z_n_clean_numeric),
    e1071::skewness(z_n_clean_numeric, type = 1),
    e1071::skewness(z_n_clean_numeric, type = 2),
    e1071::skewness(z_n_clean_numeric, type = 3),
    # already excess kurtosis for type = 1
    e1071::kurtosis(z_n_clean_numeric, type = 1),
    e1071::kurtosis(z_n_clean_numeric, type = 2),
    e1071::kurtosis(z_n_clean_numeric, type = 3)
  )
) |>
  mutate(Value = sprintf("%.6f", Value))

kable(
  desc,
  caption = "Descriptive statistics of cleaned log returns ($z_n$).",
  booktabs = TRUE,
  align = c("l", "r"),
  escape = FALSE
) |>
  kable_styling(full_width = FALSE, position = "center")

```

```{r Element 1 old summary tools, results = "asis", echo=FALSE}
#---------- Element 1: data cleaning and standard test of normality -----------#

library(summarytools)

summary_table <- descr(
  z_n_clean,
  stats = c("mean", "sd", "skewness", "kurtosis"),
  transpose = TRUE,
  headings = FALSE,
  round.digits = 6,
  plain.ascii = FALSE,
  style = "rmarkdown"
)
```

Output of `summarytools` (the technique introduced in the lab session).
Summarytools does not output the variance, but we can calculate by squaring the
standard deviation. Output from `summarytools` for variance is
`r formatC(summary_table$Std.Dev^2, format = "fg", digits = 4)` , compared to
`r formatC(var(z_n_clean_numeric), format = "fg", digits = 4)` from `e1071`.

```{r, results = "asis", echo=FALSE}
print(summary_table)
# summary_table$Std.Dev^2
# boxplot(z_n)


# # COVID: 10/04/2020-19/05/2020
#
# IXIC_ad <- IXIC_ad[
#   !((index(IXIC_ad) >= "2020-04-10") &
#     (index(IXIC_ad) <= "2020-05-19"))
# ]
#
#
# z_n = na.omit(log(IXIC_ad / dplyr::lag(IXIC_ad)))

# plot(z_n)
# boxplot(z_n)
```

_Note how the skewness and kurtosis from `summarytools` match the type 3
skewness and kurtosis from `e1071`._

### Test of normality

The Anderson-Darling test was chosen to test the normality of these data as is
it known to be more sensitive to deviations in the tails of the distribution.
Given that financial log-returns are widely theorised to be leptokurtic
("fat-tailed"), the Anderson-Darling test is a suitable test for detecting the
exact type of non-normality that is expected.

Empirical studies consistently find that asset returns (in particular, daily
log-returns) are leptokurtic, with a higher peak and heavier tails than the
normal distribution. This has been notably documented by @Mandelbrot1963 and
@Fama1965, in addition to being reported as a 'stylized empirical fact' by
@Cont01022001^[See section 4.1 in @Cont01022001 for a concise summary. BE note:
consider removing this footnote].

```{r Test of normality}
library(nortest)
ad_test_result <- ad.test(z_n_clean)
ad_test_result
```

The null hypothesis ($H_0$) for the Anderson-Darling test is that the data is
normally distributed. The test yields a statistic of A =
`r round(ad_test_result$statistic, 1)` with a p-value of
`r formatC(ad_test_result$p.value, format = "e", digits = 2)`. We therefore
reject $H_0$ in favour of $H_1$. The Anderson-Darling test provides strong
evidence that the $z_n$ data are not normally distributed.

## Investigation of normality by resampling

### Instructions (to be removed later)

You are going to investigate whether the difference between your sample and a
sample from a normal distribution becomes apparent when constructing confidence
intervals for sample skewness.

(a) Simulate a sample of size 13 from a Normal distribution with mean and
variance equal to the sample mean and variance you have just calculated.
Calculate the sample skewness from this simulated sample. Repeat this until you
have generated 50,000 values of $\hat{\gamma}_1$.

(b) Use resampling to obtain a sample of size 13 from your z-data. Calculate the
sample skewness from this bootstrap sample. Repeat this until you have generated
50,000 values of $\hat{\gamma}_1$.

(c) Use suitable illustrations to investigate the differences between the two
sets of $\hat{\gamma}$ values. Comment on what you observe and whether the
outcome of your tests confirms the result of the test of normality.

### Sample size generation

```{r Element 2 pt1 - TBC}
#---------- Element 2: investigation of normality by resampling -----------#


```


## Investigation of constant mean

### Instructions (to be removed later)

Divide your $z$ data into 14 subsamples, each representing six months. (If your
data cleansing has removed all or most of one six-month period, just omit it
from this analysis.) Compare the means of your subsamples using an appropriate
illustration, supported by at least one statistical test. [Note that Analysis of
Variance (ANOVA) is based on an assumption of Normality: if you decide to use
ANOVA for your test you will need to comment on the Normality, or otherwise, of
your 7 subsamples.]

```{r Element 3 pt1 - TBC}
#---------- Element 3: investigation of constant mean -----------#


```

## Investigation of constant variance

### Instructions (to be removed later)

Use the same subsamples as for Element 3. Use a suitable illustration to compare
the spread of your subsamples. Identify the six-month period with the largest
sample variance and the six-month period with the smallest sample variance. Use
bootstrapping/resampling to construct a 95% confidence interval for $\sigma^2$
for each of these two subsamples separately. Comment on any perceived overlap
between these intervals.

```{r Element 4 pt1 - TBC}
#---------- Element 4: investigation of constant variance -----------#


```

## Independence of increments

### Instructions (to be removed later)

The Efficient Markets Hypothesis states that the price of an asset today
incorporates all the information which is available about the asset, and
therefore that any changes to the price between today and tomorrow will be based
on new, and unpredictable, information which arrives in the meantime. The
practical result of this is that $z_{n}$ should be independent of both $z_{n−1}$
and $y_{n−1}$ if the EMH is true.

Test the independence of ${z_{n}}$ and ${z_{n-1}}$ by using a contingency table
procedure. Let $q_1$, $q_2$, $q_3$ represent the sample quartiles of the $z$
data. Classify each of the $z_n$ observations according to whether it is less
than q1, between $q_1$ and $q_2$, between $q_2$ and $q_3$, or greater than
$q_3$. Now draw up the contingency table to determine whether the classification
of $z_{n}$ is dependent on the classiﬁcation of $z_{n−1}$.

```{r Element 5 pt1 - TBC}
#---------- Element 5: independence of increments -----------#


```

## General upwardness of trend

### Instructions (to be removed later)

One aspect of the data which might reﬂect a general tendency for the index to
increase might be the proportion of days when the price increment is positive.
Test whether this proportion is greater than 0.5.

Another aspect is the persistence of trends. A “positive run” is a sequence of
consecutive days when $Z_i > 0$; a “negative run” is a sequence of consecutive
days when $Z_i < 0$. (Note that a run may only be one day in length, or may be
longer than one day.) Identify the runs in your data set and test whether
positive runs have longer mean duration than negative runs.

```{r Element 6 pt1 - TBC}
#---------- Element 6: general upwardness of trend -----------#


```

# Conclusions

```{=latex}
\newpage
```

# Appendix

## R Code Documentation

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

## Generative AI Declaration

Generative AI tools were used to assist with project planning and methodological
validation throughout this project.

### Tools Used

Google Gemini 2.5 Pro (Deep Research), ChatGPT 5

### Prompts used

The GenAI tools were provided with the 'Coursework Group 7.pdf', our R code, and
the following queries (non-exhaustive, but we have included an example of each
type of query used):

- "Can you help with outlining the steps for this project."
- "...the top of both graphs in r are slightly different. (Pasting in R code.)
  Can this be fixed for the plot area?"
- ...

### Use of GenAI output

The GenAI tools served as a additional group member. Notably, the GenAI output:

- Helped debug plotting issues.
- Raised potential statistical traps (e.g., the "ANOVA trap" and the "data
  cleansing order" trap).
- Suggested appropriate non-parametric tests (Kruskal-Wallis, Wilcoxon Rank-Sum)
  where the assumptions of parametric tests (ANOVA, t-test) were violated.
- Recommended specific R packages (nortest, e1071) and functions (ad.test,
  kruskal.test) best suited for each task.
- Commented on the structure for the R code and the final report narrative.

### Changes made following GenAI use:

The AI's suggestions were critically reviewed and debated by the group. Plotting
formatting in ggplot, ... (add others) and... were all implemented following an
iterative feedback cycle.

The R code for implementation was written and debugged by the group. GenAI tools
were used, and thought of, in the capacity of an additional group member. All
final analysis, interpretation, and writing in this report were generated by
(human) group members.

---
